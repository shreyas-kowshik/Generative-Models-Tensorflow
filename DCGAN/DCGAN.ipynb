{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "from ops import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Read the data and split the values\n",
    "        '''\n",
    "        data = pd.read_csv('mnist_train.csv')\n",
    "        self.data = np.array(data.iloc[:2,1:],dtype=np.float32) # Train only on 7000 images\n",
    "        del data\n",
    "        \n",
    "        self.data = self.data.reshape([-1,28,28,1])\n",
    "        \n",
    "        self.normalize()\n",
    "    \n",
    "    def normalize(self):\n",
    "        # self.data+=np.random.randn(self.data.shape[0],self.data.shape[1],self.data.shape[2],self.data.shape[3])\n",
    "        self.data = (self.data/127.5) - 1.0\n",
    "    \n",
    "    def denormalize(self, batch):\n",
    "        batch = (batch + 1.0)*127.5\n",
    "        return batch\n",
    "    \n",
    "    def save_batch(self,batch,idx):\n",
    "        print(np.min(batch[0]))\n",
    "        print(np.max(batch[0]))\n",
    "        \n",
    "        batch = self.denormalize(batch)\n",
    "        batch = batch.astype(np.uint8)\n",
    "        \n",
    "        # Save the images\n",
    "        save_dir = 'Generated-Images'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        \n",
    "        if not os.path.exists(save_dir + '/' + str(idx)):\n",
    "            os.mkdir(save_dir + '/' + str(idx))\n",
    "        \n",
    "        save_dir = save_dir + '/' + str(idx) + '/'\n",
    "        \n",
    "        print(np.min(batch[0]))\n",
    "        print(np.max(batch[0]))\n",
    "        \n",
    "        for i in range(batch.shape[0]):\n",
    "            cv2.imwrite(save_dir + str(i) + '.png',batch[i])\n",
    "        \n",
    "        \n",
    "    def get_minibatches(self, batch_size):\n",
    "        perms = np.random.permutation(self.data.shape[0])\n",
    "        self.data = self.data[perms]\n",
    "        \n",
    "        num_batches = int(self.data.shape[0]/batch_size)\n",
    "        \n",
    "        batches = []\n",
    "        for idx in range(num_batches):\n",
    "            start_idx = idx*batch_size\n",
    "            end_idx = (idx+1)*batch_size\n",
    "            \n",
    "            batches.append(self.data[start_idx:end_idx,:])\n",
    "        \n",
    "        frac_batches = self.data.shape[0]/batch_size\n",
    "        \n",
    "        if math.floor(frac_batches) != math.ceil(frac_batches):\n",
    "            start_idx = num_batches*batch_size\n",
    "            batches.append(self.data[start_idx:,:])\n",
    "        \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(x, is_train,reuse,name):\n",
    "        with tf.variable_scope(name,reuse=reuse):\n",
    "            ###  First Conv Block ###\n",
    "            x = pad(x,[[0,0],[1,1],[1,1],[0,0]])\n",
    "            x = conv2d(x,3,32,1,padding='VALID',name='conv1')\n",
    "            x = lrelu(x,name='a1')            \n",
    "            x = avg_pool(x,2,2,padding='VALID',name='pool1')\n",
    "              \n",
    "            ### Second Conv Block ###\n",
    "            x = bn(x,is_train=is_train,name='bn2')\n",
    "            x = pad(x,[[0,0],[2,2],[2,2],[0,0]])\n",
    "            x = conv2d(x,4,64,1,padding='VALID',name='conv2')\n",
    "            x = lrelu(x,name='a2')            \n",
    "            x = avg_pool(x,2,2,padding='VALID',name='pool2')\n",
    "            \n",
    "            ### Second Conv Block ###\n",
    "            x = bn(x,is_train=is_train,name='bn2_1')\n",
    "            x = pad(x,[[0,0],[2,2],[2,2],[0,0]])\n",
    "            x = conv2d(x,4,128,1,padding='VALID',name='conv2_1')\n",
    "            x = lrelu(x,name='a2_1')\n",
    "            # x = max_pool(x,2,2,padding='VALID',name='pool2_1')\n",
    "            x = avg_pool(x,2,2,padding='VALID',name='pool2_1')\n",
    "            \n",
    "            ### Flatten ###\n",
    "            x = flatten(x)\n",
    "            x = bn(x,is_train=is_train,name='bn3')\n",
    "            x = dense(x,4*4*64,name='dense3')\n",
    "            x = lrelu(x,name='a3')\n",
    "\n",
    "            x = bn(x,is_train=is_train,name='bn4')\n",
    "            x = dense(x,4*4*32,name='dense4')\n",
    "            x = lrelu(x,name='a4')\n",
    "            \n",
    "            x = bn(x,is_train=is_train,name='bn4_1')\n",
    "            x = dense(x,4*4*16,name='dense4_1')\n",
    "            x = lrelu(x,name='a4_1')\n",
    "            \n",
    "            x = bn(x,is_train=is_train,name='bn5')\n",
    "            x = dense(x,1,name='dense5')\n",
    "\n",
    "            out = x\n",
    "            prob = tf.nn.sigmoid(out,name='Prob')\n",
    "            \n",
    "            return prob    \n",
    "\n",
    "def Generator(x,is_train,noise_dim,name='Generator'):\n",
    "        with tf.variable_scope(name):\n",
    "            x = dense(x,7*7*8,name='dense1')\n",
    "            x = relu(x,name='a1')\n",
    "            \n",
    "            x = bn(x,is_train=is_train,name='bn1_1')\n",
    "            x = dense(x,3*3*16,name='dense1_1')\n",
    "            x = lrelu(x,name='a1_1')\n",
    "            \n",
    "            x = bn(x,is_train=is_train,name='bn2')\n",
    "            x = dense(x,3*3*32,name='dense2')\n",
    "            x = lrelu(x,name='a2')\n",
    "            \n",
    "            x = bn(x,is_train=is_train,name='bn2_1')\n",
    "            x = dense(x,3*3*64,name='dense2_1')\n",
    "            x = lrelu(x,name='a2_1')\n",
    "            \n",
    "            x = bn(x,is_train=is_train,name='bn2_2')\n",
    "            x = dense(x,3*3*128,name='dense2_2')\n",
    "            x = lrelu(x,name='a2_2')\n",
    "            \n",
    "            x = tf.reshape(x,[-1,3,3,128])\n",
    "            x = bn(x,is_train=is_train,name='bn3_0')\n",
    "            x = D_conv2d(x,3,64,2,padding='VALID',name='dconv3_0')\n",
    "            x = lrelu(x,name='a3_0')\n",
    "            \n",
    "            x = bn(x,is_train=is_train,name='bn3')\n",
    "            x = D_conv2d(x,4,64,2,name='dconv3')\n",
    "            x = lrelu(x,name='a3')\n",
    "\n",
    "            x = D_conv2d(x,4,1,2,name='dconv3_1')\n",
    "            x = tanh(x,name='a3_1')\n",
    "            \n",
    "            out = x\n",
    "            return out      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    def __init__(self,noise_dim,batch_size,d_lr=1e-3,g_lr=1e-3,d_iter=1,g_iter=1,\\\n",
    "                 num_epochs=30000,W=28,H=28,C=1,summary_path='tensorboard',name='DCGAN'):\n",
    "        self.noise_dim = noise_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.d_lr = d_lr\n",
    "        self.g_lr = g_lr\n",
    "        self.d_iter = d_iter\n",
    "        self.g_iter = g_iter\n",
    "        self.num_epochs = num_epochs\n",
    "        self.name = name\n",
    "        \n",
    "        # Image Parameters\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        self.C = C\n",
    "        \n",
    "        # Dataset\n",
    "        self.dataset = Dataset()\n",
    "        \n",
    "        self.summary_path = summary_path\n",
    "        if not os.path.exists(self.summary_path):\n",
    "            os.mkdir(self.summary_path)\n",
    "\n",
    "        self.writer = tf.summary.FileWriter(self.summary_path)\n",
    "        self.build_model()\n",
    "        \n",
    "    def log_variables(self):\n",
    "        '''\n",
    "        Summaries For Variables to add in tensorboard for visualisation\n",
    "        '''\n",
    "        tf.summary.scalar('Discriminator_Loss',self.dloss_)\n",
    "        tf.summary.scalar('Generator_Loss',self.gloss_)\n",
    "        \n",
    "        # Add the gradients to the summary \n",
    "#         d_grads = self.D_optimiser.compute_gradients(self.D_loss)\n",
    "#         g_grads = self.G_optimiser.compute_gradients(self.G_loss)\n",
    "                    \n",
    "#         for index,grads in enumerate(d_grads):\n",
    "#             tf.summary.histogram(\"{}-D_grad\".format(d_grads[index][1].name), d_grads[index])\n",
    "                            \n",
    "#         for index,grads in enumerate(g_grads):\n",
    "#             tf.summary.histogram(\"{}-G_grad\".format(g_grads[index][1].name), g_grads[index])\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        tf.reset_default_graph()\n",
    "        with tf.variable_scope(self.name):\n",
    "            with tf.name_scope('Placeholders'):\n",
    "                self.inputs = tf.placeholder(shape=[None,self.W,self.H,self.C],name='inputs',dtype=tf.float32)\n",
    "                self.is_train = tf.placeholder(name='is_train',dtype=tf.bool)\n",
    "                self.noise = tf.placeholder(shape=[None,self.noise_dim],name='noise',dtype=tf.float32)\n",
    "                \n",
    "                # Placeholders for tensorboard visualisation of losses\n",
    "                self.dloss_ = tf.placeholder(name='dloss_',dtype=tf.float32)\n",
    "                self.gloss_ = tf.placeholder(name='gloss_',dtype=tf.float32)\n",
    "\n",
    "            # Real logits\n",
    "            self.D_real = Discriminator(self.inputs,self.is_train,reuse=False,name='Dis')\n",
    "            \n",
    "            # Fake images\n",
    "            self.fake_images = Generator(self.noise,self.is_train,self.noise_dim,name='Gen')\n",
    "            \n",
    "            # Fake logits\n",
    "            self.D_fake = Discriminator(self.fake_images,self.is_train,reuse=True,name='Dis')\n",
    "\n",
    "            # Define the losses\n",
    "            # D_real must tend to 1.0 and D_fake must tend to 0.0\n",
    "            # self.D_loss = tf.reduce_mean(-tf.log(self.D_real) - tf.log(1.0 - self.D_fake))\n",
    "            # D_fake must tend to 1.0\n",
    "            # self.G_loss = tf.reduce_mean(-tf.log(self.D_fake))\n",
    "            # self.D_loss = tf.reduce_mean(tf.squared_difference(self.D_real,1.0) + tf.square(self.D_fake))\n",
    "            # self.G_loss = tf.reduce_mean(tf.squared_difference(self.D_fake,1.0))\n",
    "            self.D_loss = tf.reduce_mean(self.D_fake) - tf.reduce_mean(self.D_real)\n",
    "            self.G_loss = -tf.reduce_mean(self.D_fake)\n",
    "            \n",
    "            # Seperate the variables\n",
    "            var = tf.trainable_variables()\n",
    "            self.d_vars = [v for v in var if 'Dis' in v.name]\n",
    "            self.g_vars = [v for v in var if 'Gen' in v.name]\n",
    "            \n",
    "            self.d_clip = [v.assign(tf.clip_by_value(v, -0.1, 0.1)) for v in self.d_vars]\n",
    "            \n",
    "            # Define the optimizers\n",
    "            self.D_optimiser = tf.train.AdamOptimizer(self.d_lr)\n",
    "            self.G_optimiser = tf.train.AdamOptimizer(self.g_lr)\n",
    "            \n",
    "            self.D_optim = self.D_optimiser.minimize(self.D_loss,var_list=self.d_vars)\n",
    "            self.G_optim = self.G_optimiser.minimize(self.G_loss,var_list=self.g_vars)\n",
    "\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "                        \n",
    "            self.log_variables()\n",
    "            \n",
    "            # Write the graph summary to disk\n",
    "            self.writer.add_graph(self.sess.graph)\n",
    "            \n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            dloss_epoch = 0.0\n",
    "            gloss_epoch = 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch in self.dataset.get_minibatches(self.batch_size):\n",
    "                saved = False\n",
    "                \n",
    "                # batch corresponds to the real image\n",
    "                # Sample some noise\n",
    "                noise = np.random.uniform(-1.0,1.0,size=[batch.shape[0],self.noise_dim]).astype(np.float32)\n",
    "                \n",
    "                # Train the discriminator\n",
    "                for _ in range(self.d_iter):\n",
    "                    self.sess.run(self.d_clip)\n",
    "                    _ = self.sess.run([self.D_optim],feed_dict={self.inputs:batch,self.noise:noise,self.is_train:True})\n",
    "                \n",
    "                # Train the generator\n",
    "                for _ in range(self.g_iter):\n",
    "                    _ = self.sess.run([self.G_optim],feed_dict={self.inputs:batch,self.noise:noise,self.is_train:True})\n",
    "                \n",
    "                dloss_batch = self.sess.run(self.D_loss,feed_dict={self.inputs:batch,self.noise:noise,self.is_train:True})\n",
    "                gloss_batch = self.sess.run(self.G_loss,feed_dict={self.inputs:batch,self.noise:noise,self.is_train:True})\n",
    "                \n",
    "                # Update statistics\n",
    "                dloss_epoch+=dloss_batch\n",
    "                gloss_epoch+=gloss_batch\n",
    "                batch_count+=1\n",
    "                \n",
    "                # Generate images every 100 epochs\n",
    "                if epoch%100 == 0:\n",
    "                    if saved is False:\n",
    "                        generated_batch = self.sess.run(self.fake_images,feed_dict={self.noise:noise,self.is_train:False})\n",
    "                        generated_batch = np.array(generated_batch)\n",
    "                        self.dataset.save_batch(generated_batch,epoch)\n",
    "                        saved = True\n",
    "                        \n",
    "                    drl,dfk = self.sess.run([self.D_real,self.D_fake],feed_dict={self.inputs:batch,self.noise:noise,self.is_train:True})\n",
    "                    print('D Real : {}, D Fake : {}'.format(drl,dfk))\n",
    "                \n",
    "                print('Epoch[{}/{}] - Batch : {}'.format(epoch,self.num_epochs,batch_count))\n",
    "            \n",
    "            # Not logging gradients at the moment...\n",
    "            summary = self.sess.run(self.merged,feed_dict={self.dloss_:dloss_epoch,self.gloss_:gloss_epoch,\\\n",
    "                                                                       self.inputs:batch,self.noise:noise,self.is_train:True})\n",
    "            self.writer.add_summary(summary,epoch)\n",
    "            self.writer.flush()\n",
    "            \n",
    "            dloss_epoch/=(1.0*batch_count)\n",
    "            gloss_epoch/=(1.0*batch_count)\n",
    "            \n",
    "            print('D Loss : {}, G Loss : {}'.format(dloss_epoch,gloss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan = DCGAN(10,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0014131936\n",
      "0.005056422\n",
      "127\n",
      "128\n",
      "D Real : [[0.50396466]\n",
      " [0.4952981 ]], D Fake : [[0.49575213]\n",
      " [0.50351065]]\n",
      "Epoch[0/30000] - Batch : 1\n",
      "D Loss : 2.9802322387695312e-08, G Loss : -0.499631404876709\n",
      "Epoch[1/30000] - Batch : 1\n",
      "D Loss : -4.172325134277344e-07, G Loss : -0.4990231394767761\n",
      "Epoch[2/30000] - Batch : 1\n",
      "D Loss : -8.940696716308594e-07, G Loss : -0.4982718825340271\n",
      "Epoch[3/30000] - Batch : 1\n",
      "D Loss : -5.066394805908203e-06, G Loss : -0.4975033104419708\n",
      "Epoch[4/30000] - Batch : 1\n",
      "D Loss : -1.9788742065429688e-05, G Loss : -0.4967110753059387\n",
      "Epoch[5/30000] - Batch : 1\n",
      "D Loss : -5.245208740234375e-05, G Loss : -0.49587374925613403\n",
      "Epoch[6/30000] - Batch : 1\n",
      "D Loss : -9.629130363464355e-05, G Loss : -0.4948854148387909\n",
      "Epoch[7/30000] - Batch : 1\n",
      "D Loss : -0.00012600421905517578, G Loss : -0.4938092827796936\n",
      "Epoch[8/30000] - Batch : 1\n",
      "D Loss : -0.00018414855003356934, G Loss : -0.4925737977027893\n",
      "Epoch[9/30000] - Batch : 1\n",
      "D Loss : -0.00023114681243896484, G Loss : -0.49122920632362366\n",
      "Epoch[10/30000] - Batch : 1\n",
      "D Loss : -0.0002695024013519287, G Loss : -0.4897433817386627\n",
      "Epoch[11/30000] - Batch : 1\n",
      "D Loss : -0.00041991472244262695, G Loss : -0.48805224895477295\n",
      "Epoch[12/30000] - Batch : 1\n",
      "D Loss : -0.0005843639373779297, G Loss : -0.4862520694732666\n",
      "Epoch[13/30000] - Batch : 1\n",
      "D Loss : -0.0007396340370178223, G Loss : -0.48430418968200684\n",
      "Epoch[14/30000] - Batch : 1\n",
      "D Loss : -0.0005155205726623535, G Loss : -0.4826078712940216\n",
      "Epoch[15/30000] - Batch : 1\n",
      "D Loss : -0.0011638998985290527, G Loss : -0.48001962900161743\n",
      "Epoch[16/30000] - Batch : 1\n",
      "D Loss : -0.0013542473316192627, G Loss : -0.4777250289916992\n",
      "Epoch[17/30000] - Batch : 1\n",
      "D Loss : -0.0018393397331237793, G Loss : -0.4750296473503113\n",
      "Epoch[18/30000] - Batch : 1\n",
      "D Loss : -0.0012843608856201172, G Loss : -0.4731592535972595\n",
      "Epoch[19/30000] - Batch : 1\n",
      "D Loss : -0.0009711384773254395, G Loss : -0.47087663412094116\n",
      "Epoch[20/30000] - Batch : 1\n",
      "D Loss : -0.002425670623779297, G Loss : -0.46711504459381104\n",
      "Epoch[21/30000] - Batch : 1\n",
      "D Loss : -0.0034310221672058105, G Loss : -0.46361643075942993\n",
      "Epoch[22/30000] - Batch : 1\n",
      "D Loss : -0.003567218780517578, G Loss : -0.460725337266922\n",
      "Epoch[23/30000] - Batch : 1\n",
      "D Loss : -0.004772961139678955, G Loss : -0.4568641185760498\n",
      "Epoch[24/30000] - Batch : 1\n",
      "D Loss : -0.005606710910797119, G Loss : -0.4530256390571594\n",
      "Epoch[25/30000] - Batch : 1\n",
      "D Loss : -0.006621837615966797, G Loss : -0.44906026124954224\n",
      "Epoch[26/30000] - Batch : 1\n",
      "D Loss : -0.007555842399597168, G Loss : -0.44486504793167114\n",
      "Epoch[27/30000] - Batch : 1\n",
      "D Loss : -0.008693814277648926, G Loss : -0.44045233726501465\n",
      "Epoch[28/30000] - Batch : 1\n",
      "D Loss : -0.010290563106536865, G Loss : -0.4357411861419678\n",
      "Epoch[29/30000] - Batch : 1\n",
      "D Loss : -0.011848628520965576, G Loss : -0.4307577610015869\n",
      "Epoch[30/30000] - Batch : 1\n",
      "D Loss : -0.013451039791107178, G Loss : -0.4254890978336334\n",
      "Epoch[31/30000] - Batch : 1\n",
      "D Loss : -0.015406936407089233, G Loss : -0.41996368765830994\n",
      "Epoch[32/30000] - Batch : 1\n",
      "D Loss : -0.01764380931854248, G Loss : -0.4141777753829956\n",
      "Epoch[33/30000] - Batch : 1\n",
      "D Loss : -0.016637802124023438, G Loss : -0.41154026985168457\n",
      "Epoch[34/30000] - Batch : 1\n",
      "D Loss : -0.02248820662498474, G Loss : -0.4019089639186859\n",
      "Epoch[35/30000] - Batch : 1\n",
      "D Loss : -0.025328338146209717, G Loss : -0.3952900171279907\n",
      "Epoch[36/30000] - Batch : 1\n",
      "D Loss : -0.028078347444534302, G Loss : -0.3883886933326721\n",
      "Epoch[37/30000] - Batch : 1\n",
      "D Loss : -0.029469847679138184, G Loss : -0.381264865398407\n",
      "Epoch[38/30000] - Batch : 1\n",
      "D Loss : -0.034169137477874756, G Loss : -0.3737441897392273\n",
      "Epoch[39/30000] - Batch : 1\n",
      "D Loss : -0.03667148947715759, G Loss : -0.36613595485687256\n",
      "Epoch[40/30000] - Batch : 1\n",
      "D Loss : -0.040967583656311035, G Loss : -0.35871803760528564\n",
      "Epoch[41/30000] - Batch : 1\n",
      "D Loss : -0.0457894504070282, G Loss : -0.3506720960140228\n",
      "Epoch[42/30000] - Batch : 1\n",
      "D Loss : -0.04988113045692444, G Loss : -0.3420621156692505\n",
      "Epoch[43/30000] - Batch : 1\n",
      "D Loss : -0.053693681955337524, G Loss : -0.3335120677947998\n",
      "Epoch[44/30000] - Batch : 1\n",
      "D Loss : -0.05852225422859192, G Loss : -0.3248457610607147\n",
      "Epoch[45/30000] - Batch : 1\n",
      "D Loss : -0.06296715140342712, G Loss : -0.3167949318885803\n",
      "Epoch[46/30000] - Batch : 1\n",
      "D Loss : -0.0645887553691864, G Loss : -0.3075362741947174\n",
      "Epoch[47/30000] - Batch : 1\n",
      "D Loss : -0.07382580637931824, G Loss : -0.2986876964569092\n",
      "Epoch[48/30000] - Batch : 1\n",
      "D Loss : -0.07673367857933044, G Loss : -0.29128706455230713\n",
      "Epoch[49/30000] - Batch : 1\n",
      "D Loss : -0.08351153135299683, G Loss : -0.2811788022518158\n",
      "Epoch[50/30000] - Batch : 1\n",
      "D Loss : -0.08792132139205933, G Loss : -0.2719833552837372\n",
      "Epoch[51/30000] - Batch : 1\n",
      "D Loss : -0.08148956298828125, G Loss : -0.26259535551071167\n",
      "Epoch[52/30000] - Batch : 1\n",
      "D Loss : -0.08990132808685303, G Loss : -0.25521212816238403\n",
      "Epoch[53/30000] - Batch : 1\n",
      "D Loss : -0.09493142366409302, G Loss : -0.2559546232223511\n",
      "Epoch[54/30000] - Batch : 1\n",
      "D Loss : -0.1025787740945816, G Loss : -0.24109195172786713\n",
      "Epoch[55/30000] - Batch : 1\n",
      "D Loss : -0.11167162656784058, G Loss : -0.23028746247291565\n",
      "Epoch[56/30000] - Batch : 1\n",
      "D Loss : -0.11658021807670593, G Loss : -0.22080984711647034\n",
      "Epoch[57/30000] - Batch : 1\n",
      "D Loss : -0.10453125834465027, G Loss : -0.2122669517993927\n",
      "Epoch[58/30000] - Batch : 1\n",
      "D Loss : -0.04781457781791687, G Loss : -0.28452903032302856\n",
      "Epoch[59/30000] - Batch : 1\n",
      "D Loss : -0.12914372980594635, G Loss : -0.20537973940372467\n",
      "Epoch[60/30000] - Batch : 1\n",
      "D Loss : -0.11092476546764374, G Loss : -0.1812596470117569\n",
      "Epoch[61/30000] - Batch : 1\n",
      "D Loss : -0.1364765167236328, G Loss : -0.19183605909347534\n",
      "Epoch[62/30000] - Batch : 1\n",
      "D Loss : -0.15205614268779755, G Loss : -0.17281226813793182\n",
      "Epoch[63/30000] - Batch : 1\n",
      "D Loss : -0.15722684562206268, G Loss : -0.1658959835767746\n",
      "Epoch[64/30000] - Batch : 1\n",
      "D Loss : -0.09254398941993713, G Loss : -0.22814911603927612\n",
      "Epoch[65/30000] - Batch : 1\n",
      "D Loss : -0.16561642289161682, G Loss : -0.15340957045555115\n",
      "Epoch[66/30000] - Batch : 1\n",
      "D Loss : -0.0010049939155578613, G Loss : -0.31484749913215637\n",
      "Epoch[67/30000] - Batch : 1\n",
      "D Loss : -0.17450740933418274, G Loss : -0.1400233805179596\n",
      "Epoch[68/30000] - Batch : 1\n",
      "D Loss : -0.05000707507133484, G Loss : -0.2518705129623413\n",
      "Epoch[69/30000] - Batch : 1\n",
      "D Loss : -0.1606234908103943, G Loss : -0.13158369064331055\n",
      "Epoch[70/30000] - Batch : 1\n",
      "D Loss : -0.16972441971302032, G Loss : -0.1290179044008255\n",
      "Epoch[71/30000] - Batch : 1\n",
      "D Loss : -0.167003333568573, G Loss : -0.12523266673088074\n",
      "Epoch[72/30000] - Batch : 1\n",
      "D Loss : -0.10378660261631012, G Loss : -0.18646375834941864\n",
      "Epoch[73/30000] - Batch : 1\n",
      "D Loss : -0.10434848070144653, G Loss : -0.16332334280014038\n",
      "Epoch[74/30000] - Batch : 1\n",
      "D Loss : -0.14552077651023865, G Loss : -0.11900684237480164\n",
      "Epoch[75/30000] - Batch : 1\n",
      "D Loss : -0.1524258852005005, G Loss : -0.11817523837089539\n",
      "Epoch[76/30000] - Batch : 1\n",
      "D Loss : -0.15460503101348877, G Loss : -0.1167442724108696\n",
      "Epoch[77/30000] - Batch : 1\n",
      "D Loss : -0.15823085606098175, G Loss : -0.11955131590366364\n",
      "Epoch[78/30000] - Batch : 1\n",
      "D Loss : -0.1573539525270462, G Loss : -0.11931480467319489\n",
      "Epoch[79/30000] - Batch : 1\n",
      "D Loss : -0.156078040599823, G Loss : -0.12136104702949524\n",
      "Epoch[80/30000] - Batch : 1\n",
      "D Loss : -0.0721132904291153, G Loss : -0.20277978479862213\n",
      "Epoch[81/30000] - Batch : 1\n",
      "D Loss : -0.1556556224822998, G Loss : -0.11952117830514908\n",
      "Epoch[82/30000] - Batch : 1\n",
      "D Loss : -0.16206683218479156, G Loss : -0.11879758536815643\n",
      "Epoch[83/30000] - Batch : 1\n",
      "D Loss : -0.1625174731016159, G Loss : -0.12073640525341034\n",
      "Epoch[84/30000] - Batch : 1\n",
      "D Loss : -0.1553851217031479, G Loss : -0.13305895030498505\n",
      "Epoch[85/30000] - Batch : 1\n",
      "D Loss : -0.16138193011283875, G Loss : -0.1304527223110199\n",
      "Epoch[86/30000] - Batch : 1\n",
      "D Loss : -0.011823266744613647, G Loss : -0.2834334373474121\n",
      "Epoch[87/30000] - Batch : 1\n",
      "D Loss : -0.17083117365837097, G Loss : -0.12805461883544922\n",
      "Epoch[88/30000] - Batch : 1\n",
      "D Loss : -0.0005868375301361084, G Loss : -0.3008362352848053\n",
      "Epoch[89/30000] - Batch : 1\n",
      "D Loss : -0.061489030718803406, G Loss : -0.2154306024312973\n",
      "Epoch[90/30000] - Batch : 1\n",
      "D Loss : -0.1488105207681656, G Loss : -0.13335047662258148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[91/30000] - Batch : 1\n",
      "D Loss : -0.14774870872497559, G Loss : -0.13544410467147827\n",
      "Epoch[92/30000] - Batch : 1\n",
      "D Loss : -0.06167669594287872, G Loss : -0.22474460303783417\n",
      "Epoch[93/30000] - Batch : 1\n",
      "D Loss : -0.12976160645484924, G Loss : -0.15505850315093994\n",
      "Epoch[94/30000] - Batch : 1\n",
      "D Loss : -0.14676792919635773, G Loss : -0.14323993027210236\n",
      "Epoch[95/30000] - Batch : 1\n",
      "D Loss : -0.07579691708087921, G Loss : -0.2163187712430954\n",
      "Epoch[96/30000] - Batch : 1\n",
      "D Loss : -0.09046304225921631, G Loss : -0.19469201564788818\n",
      "Epoch[97/30000] - Batch : 1\n",
      "D Loss : -0.07351845502853394, G Loss : -0.19413766264915466\n",
      "Epoch[98/30000] - Batch : 1\n",
      "D Loss : -0.11391332745552063, G Loss : -0.1601496934890747\n",
      "Epoch[99/30000] - Batch : 1\n",
      "D Loss : -0.1204550713300705, G Loss : -0.15622667968273163\n",
      "-0.3594756\n",
      "0.9928111\n",
      "81\n",
      "254\n",
      "D Real : [[0.02855554]\n",
      " [0.5156024 ]], D Fake : [[0.35009348]\n",
      " [0.05489494]]\n",
      "Epoch[100/30000] - Batch : 1\n",
      "D Loss : -0.06958474218845367, G Loss : -0.20249421894550323\n",
      "Epoch[101/30000] - Batch : 1\n",
      "D Loss : -0.1098451167345047, G Loss : -0.15818162262439728\n",
      "Epoch[102/30000] - Batch : 1\n",
      "D Loss : -0.06368835270404816, G Loss : -0.20939724147319794\n",
      "Epoch[103/30000] - Batch : 1\n",
      "D Loss : -0.12083226442337036, G Loss : -0.15432780981063843\n",
      "Epoch[104/30000] - Batch : 1\n",
      "D Loss : -0.07346950471401215, G Loss : -0.2014450877904892\n",
      "Epoch[105/30000] - Batch : 1\n",
      "D Loss : -0.06401370465755463, G Loss : -0.21078844368457794\n",
      "Epoch[106/30000] - Batch : 1\n",
      "D Loss : -0.10533551871776581, G Loss : -0.16591854393482208\n",
      "Epoch[107/30000] - Batch : 1\n",
      "D Loss : -0.06170418858528137, G Loss : -0.21114051342010498\n",
      "Epoch[108/30000] - Batch : 1\n",
      "D Loss : -0.10488435626029968, G Loss : -0.17075783014297485\n",
      "Epoch[109/30000] - Batch : 1\n",
      "D Loss : -0.06829002499580383, G Loss : -0.20458129048347473\n",
      "Epoch[110/30000] - Batch : 1\n",
      "D Loss : -0.11131705343723297, G Loss : -0.16499485075473785\n",
      "Epoch[111/30000] - Batch : 1\n",
      "D Loss : -0.09401752054691315, G Loss : -0.18009983003139496\n",
      "Epoch[112/30000] - Batch : 1\n",
      "D Loss : -0.10412503778934479, G Loss : -0.1740870624780655\n",
      "Epoch[113/30000] - Batch : 1\n",
      "D Loss : -0.10763463377952576, G Loss : -0.17281579971313477\n",
      "Epoch[114/30000] - Batch : 1\n",
      "D Loss : -0.11931312084197998, G Loss : -0.16407090425491333\n",
      "Epoch[115/30000] - Batch : 1\n",
      "D Loss : -0.10676029324531555, G Loss : -0.1714296042919159\n",
      "Epoch[116/30000] - Batch : 1\n",
      "D Loss : -0.11477205157279968, G Loss : -0.16693884134292603\n",
      "Epoch[117/30000] - Batch : 1\n",
      "D Loss : -0.10869714617729187, G Loss : -0.17416566610336304\n",
      "Epoch[118/30000] - Batch : 1\n",
      "D Loss : -0.10948637127876282, G Loss : -0.17460721731185913\n",
      "Epoch[119/30000] - Batch : 1\n",
      "D Loss : -0.11045567691326141, G Loss : -0.17440499365329742\n",
      "Epoch[120/30000] - Batch : 1\n",
      "D Loss : -0.11163732409477234, G Loss : -0.17474478483200073\n",
      "Epoch[121/30000] - Batch : 1\n",
      "D Loss : -0.11857973039150238, G Loss : -0.17053647339344025\n",
      "Epoch[122/30000] - Batch : 1\n",
      "D Loss : -0.12093064188957214, G Loss : -0.1694493591785431\n",
      "Epoch[123/30000] - Batch : 1\n",
      "D Loss : -0.08797964453697205, G Loss : -0.2036762833595276\n",
      "Epoch[124/30000] - Batch : 1\n",
      "D Loss : -0.11558300256729126, G Loss : -0.1771300733089447\n",
      "Epoch[125/30000] - Batch : 1\n",
      "D Loss : -0.11200858652591705, G Loss : -0.1775081902742386\n",
      "Epoch[126/30000] - Batch : 1\n",
      "D Loss : -0.11965429782867432, G Loss : -0.1709461212158203\n",
      "Epoch[127/30000] - Batch : 1\n",
      "D Loss : -0.11291103065013885, G Loss : -0.17861033976078033\n",
      "Epoch[128/30000] - Batch : 1\n",
      "D Loss : -0.11398021876811981, G Loss : -0.1778266876935959\n",
      "Epoch[129/30000] - Batch : 1\n",
      "D Loss : -0.11512412130832672, G Loss : -0.17830555140972137\n",
      "Epoch[130/30000] - Batch : 1\n",
      "D Loss : -0.1158801019191742, G Loss : -0.17831674218177795\n",
      "Epoch[131/30000] - Batch : 1\n",
      "D Loss : -0.11646351218223572, G Loss : -0.1783967912197113\n",
      "Epoch[132/30000] - Batch : 1\n",
      "D Loss : -0.1168820708990097, G Loss : -0.17861159145832062\n",
      "Epoch[133/30000] - Batch : 1\n",
      "D Loss : -0.11800746619701385, G Loss : -0.17801706492900848\n",
      "Epoch[134/30000] - Batch : 1\n",
      "D Loss : -0.11857321858406067, G Loss : -0.177932471036911\n",
      "Epoch[135/30000] - Batch : 1\n",
      "D Loss : -0.12609882652759552, G Loss : -0.17090971767902374\n",
      "Epoch[136/30000] - Batch : 1\n",
      "D Loss : -0.11944624781608582, G Loss : -0.17791318893432617\n",
      "Epoch[137/30000] - Batch : 1\n",
      "D Loss : -0.11921800673007965, G Loss : -0.17849819362163544\n",
      "Epoch[138/30000] - Batch : 1\n",
      "D Loss : -0.11956611275672913, G Loss : -0.17835748195648193\n",
      "Epoch[139/30000] - Batch : 1\n",
      "D Loss : -0.12070766091346741, G Loss : -0.17738038301467896\n",
      "Epoch[140/30000] - Batch : 1\n",
      "D Loss : -0.1292521059513092, G Loss : -0.16924414038658142\n",
      "Epoch[141/30000] - Batch : 1\n",
      "D Loss : -0.12243656814098358, G Loss : -0.17625992000102997\n",
      "Epoch[142/30000] - Batch : 1\n",
      "D Loss : -0.12541630864143372, G Loss : -0.1734510362148285\n",
      "Epoch[143/30000] - Batch : 1\n",
      "D Loss : -0.12487006187438965, G Loss : -0.17411959171295166\n",
      "Epoch[144/30000] - Batch : 1\n",
      "D Loss : -0.12655074894428253, G Loss : -0.17253531515598297\n",
      "Epoch[145/30000] - Batch : 1\n",
      "D Loss : -0.1323033571243286, G Loss : -0.16691219806671143\n",
      "Epoch[146/30000] - Batch : 1\n",
      "D Loss : -0.12757281959056854, G Loss : -0.17007185518741608\n",
      "Epoch[147/30000] - Batch : 1\n",
      "D Loss : -0.133175790309906, G Loss : -0.16605067253112793\n",
      "Epoch[148/30000] - Batch : 1\n",
      "D Loss : -0.13051539659500122, G Loss : -0.16928821802139282\n",
      "Epoch[149/30000] - Batch : 1\n",
      "D Loss : -0.1307315230369568, G Loss : -0.16928428411483765\n",
      "Epoch[150/30000] - Batch : 1\n",
      "D Loss : -0.13160207867622375, G Loss : -0.1684401035308838\n",
      "Epoch[151/30000] - Batch : 1\n",
      "D Loss : -0.13155004382133484, G Loss : -0.1684611439704895\n",
      "Epoch[152/30000] - Batch : 1\n",
      "D Loss : -0.1297500729560852, G Loss : -0.1681888997554779\n",
      "Epoch[153/30000] - Batch : 1\n",
      "D Loss : -0.12983503937721252, G Loss : -0.167947918176651\n",
      "Epoch[154/30000] - Batch : 1\n",
      "D Loss : -0.12830767035484314, G Loss : -0.16773119568824768\n",
      "Epoch[155/30000] - Batch : 1\n",
      "D Loss : -0.130835622549057, G Loss : -0.16321638226509094\n",
      "Epoch[156/30000] - Batch : 1\n",
      "D Loss : -0.1317371129989624, G Loss : -0.16303175687789917\n",
      "Epoch[157/30000] - Batch : 1\n",
      "D Loss : -0.13017088174819946, G Loss : -0.16698187589645386\n",
      "Epoch[158/30000] - Batch : 1\n",
      "D Loss : -0.0648762583732605, G Loss : -0.2216825783252716\n",
      "Epoch[159/30000] - Batch : 1\n",
      "D Loss : -0.10906413197517395, G Loss : -0.16565001010894775\n",
      "Epoch[160/30000] - Batch : 1\n",
      "D Loss : -0.09795224666595459, G Loss : -0.184087336063385\n",
      "Epoch[161/30000] - Batch : 1\n",
      "D Loss : -0.11643323302268982, G Loss : -0.16610610485076904\n",
      "Epoch[162/30000] - Batch : 1\n",
      "D Loss : -0.02931356430053711, G Loss : -0.24059519171714783\n",
      "Epoch[163/30000] - Batch : 1\n",
      "D Loss : -0.011764049530029297, G Loss : -0.260741263628006\n",
      "Epoch[164/30000] - Batch : 1\n",
      "D Loss : -0.021409660577774048, G Loss : -0.21505969762802124\n",
      "Epoch[165/30000] - Batch : 1\n",
      "D Loss : -0.019332081079483032, G Loss : -0.22055917978286743\n",
      "Epoch[166/30000] - Batch : 1\n",
      "D Loss : -0.08482402563095093, G Loss : -0.1674562394618988\n",
      "Epoch[167/30000] - Batch : 1\n",
      "D Loss : 0.004623502492904663, G Loss : -0.2617180645465851\n",
      "Epoch[168/30000] - Batch : 1\n",
      "D Loss : 0.003590792417526245, G Loss : -0.2629545331001282\n",
      "Epoch[169/30000] - Batch : 1\n",
      "D Loss : -0.07369685173034668, G Loss : -0.18708685040473938\n",
      "Epoch[170/30000] - Batch : 1\n",
      "D Loss : -0.07923108339309692, G Loss : -0.1821274757385254\n",
      "Epoch[171/30000] - Batch : 1\n",
      "D Loss : -0.09575515985488892, G Loss : -0.16479015350341797\n",
      "Epoch[172/30000] - Batch : 1\n",
      "D Loss : -0.09909304976463318, G Loss : -0.16459491848945618\n",
      "Epoch[173/30000] - Batch : 1\n",
      "D Loss : -0.09841224551200867, G Loss : -0.16501346230506897\n",
      "Epoch[174/30000] - Batch : 1\n",
      "D Loss : -0.09923705458641052, G Loss : -0.16560962796211243\n",
      "Epoch[175/30000] - Batch : 1\n",
      "D Loss : -0.09491419792175293, G Loss : -0.16729813814163208\n",
      "Epoch[176/30000] - Batch : 1\n",
      "D Loss : -0.0945008248090744, G Loss : -0.1679203063249588\n",
      "Epoch[177/30000] - Batch : 1\n",
      "D Loss : -0.0947231650352478, G Loss : -0.1708482801914215\n",
      "Epoch[178/30000] - Batch : 1\n",
      "D Loss : -0.05781705677509308, G Loss : -0.21027056872844696\n",
      "Epoch[179/30000] - Batch : 1\n",
      "D Loss : -0.09993383288383484, G Loss : -0.1721077263355255\n",
      "Epoch[180/30000] - Batch : 1\n",
      "D Loss : -0.10152938961982727, G Loss : -0.17288774251937866\n",
      "Epoch[181/30000] - Batch : 1\n",
      "D Loss : -0.10246753692626953, G Loss : -0.17395401000976562\n",
      "Epoch[182/30000] - Batch : 1\n",
      "D Loss : -0.1031779795885086, G Loss : -0.17514948546886444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[183/30000] - Batch : 1\n",
      "D Loss : -0.10082001984119415, G Loss : -0.1787712126970291\n",
      "Epoch[184/30000] - Batch : 1\n",
      "D Loss : -0.08989641070365906, G Loss : -0.17714691162109375\n",
      "Epoch[185/30000] - Batch : 1\n",
      "D Loss : -0.09137424826622009, G Loss : -0.1778411567211151\n",
      "Epoch[186/30000] - Batch : 1\n",
      "D Loss : -0.08863890171051025, G Loss : -0.18273374438285828\n",
      "Epoch[187/30000] - Batch : 1\n",
      "D Loss : -0.0933539867401123, G Loss : -0.17941460013389587\n",
      "Epoch[188/30000] - Batch : 1\n",
      "D Loss : -0.09381020069122314, G Loss : -0.18018370866775513\n",
      "Epoch[189/30000] - Batch : 1\n",
      "D Loss : -0.09378644824028015, G Loss : -0.18155908584594727\n",
      "Epoch[190/30000] - Batch : 1\n",
      "D Loss : -0.09532412886619568, G Loss : -0.18138423562049866\n",
      "Epoch[191/30000] - Batch : 1\n",
      "D Loss : -0.09784603118896484, G Loss : -0.18198400735855103\n",
      "Epoch[192/30000] - Batch : 1\n",
      "D Loss : -0.06811545789241791, G Loss : -0.2104954868555069\n",
      "Epoch[193/30000] - Batch : 1\n",
      "D Loss : -0.09688693284988403, G Loss : -0.18262994289398193\n",
      "Epoch[194/30000] - Batch : 1\n",
      "D Loss : -0.09746751189231873, G Loss : -0.1828734278678894\n",
      "Epoch[195/30000] - Batch : 1\n",
      "D Loss : -0.09360653162002563, G Loss : -0.18357744812965393\n",
      "Epoch[196/30000] - Batch : 1\n",
      "D Loss : -0.09450635313987732, G Loss : -0.1830752193927765\n",
      "Epoch[197/30000] - Batch : 1\n",
      "D Loss : -0.09643463790416718, G Loss : -0.18322204053401947\n",
      "Epoch[198/30000] - Batch : 1\n",
      "D Loss : -0.09350830316543579, G Loss : -0.18311241269111633\n",
      "Epoch[199/30000] - Batch : 1\n",
      "D Loss : -0.09362666308879852, G Loss : -0.1828024536371231\n",
      "-0.9997197\n",
      "0.9999798\n",
      "0\n",
      "254\n",
      "D Real : [[0.04484785]\n",
      " [0.5153972 ]], D Fake : [[0.19245908]\n",
      " [0.17323428]]\n",
      "Epoch[200/30000] - Batch : 1\n",
      "D Loss : -0.09727583825588226, G Loss : -0.18284668028354645\n",
      "Epoch[201/30000] - Batch : 1\n",
      "D Loss : -0.09445768594741821, G Loss : -0.18479707837104797\n",
      "Epoch[202/30000] - Batch : 1\n",
      "D Loss : -0.09720999002456665, G Loss : -0.1824449896812439\n",
      "Epoch[203/30000] - Batch : 1\n",
      "D Loss : -0.070286825299263, G Loss : -0.2049071341753006\n",
      "Epoch[204/30000] - Batch : 1\n",
      "D Loss : -0.08998142182826996, G Loss : -0.18181602656841278\n",
      "Epoch[205/30000] - Batch : 1\n",
      "D Loss : -0.06155569851398468, G Loss : -0.20496217906475067\n",
      "Epoch[206/30000] - Batch : 1\n",
      "D Loss : -0.08796408772468567, G Loss : -0.18124642968177795\n",
      "Epoch[207/30000] - Batch : 1\n",
      "D Loss : -0.07302959263324738, G Loss : -0.19136865437030792\n",
      "Epoch[208/30000] - Batch : 1\n",
      "D Loss : -0.07886166870594025, G Loss : -0.1817334145307541\n",
      "Epoch[209/30000] - Batch : 1\n",
      "D Loss : -0.08521342277526855, G Loss : -0.18054470419883728\n",
      "Epoch[210/30000] - Batch : 1\n",
      "D Loss : -0.08431202173233032, G Loss : -0.180373877286911\n",
      "Epoch[211/30000] - Batch : 1\n",
      "D Loss : -0.08672469854354858, G Loss : -0.1802290976047516\n",
      "Epoch[212/30000] - Batch : 1\n",
      "D Loss : -0.06394390761852264, G Loss : -0.20329786837100983\n",
      "Epoch[213/30000] - Batch : 1\n",
      "D Loss : -0.08699387311935425, G Loss : -0.18038082122802734\n",
      "Epoch[214/30000] - Batch : 1\n",
      "D Loss : -0.08739282190799713, G Loss : -0.18013368546962738\n",
      "Epoch[215/30000] - Batch : 1\n",
      "D Loss : -0.08837619423866272, G Loss : -0.18004587292671204\n",
      "Epoch[216/30000] - Batch : 1\n",
      "D Loss : -0.0843762755393982, G Loss : -0.18478092551231384\n",
      "Epoch[217/30000] - Batch : 1\n",
      "D Loss : -0.08949041366577148, G Loss : -0.1804746687412262\n",
      "Epoch[218/30000] - Batch : 1\n",
      "D Loss : -0.03567647933959961, G Loss : -0.23487582802772522\n",
      "Epoch[219/30000] - Batch : 1\n",
      "D Loss : -0.09009937942028046, G Loss : -0.18057261407375336\n",
      "Epoch[220/30000] - Batch : 1\n",
      "D Loss : -0.08848094940185547, G Loss : -0.18057280778884888\n",
      "Epoch[221/30000] - Batch : 1\n",
      "D Loss : -0.0889119952917099, G Loss : -0.18075324594974518\n",
      "Epoch[222/30000] - Batch : 1\n",
      "D Loss : -0.08858427405357361, G Loss : -0.1808057725429535\n",
      "Epoch[223/30000] - Batch : 1\n",
      "D Loss : -0.048699751496315, G Loss : -0.21973498165607452\n",
      "Epoch[224/30000] - Batch : 1\n",
      "D Loss : -0.08545993268489838, G Loss : -0.18098317086696625\n",
      "Epoch[225/30000] - Batch : 1\n",
      "D Loss : -0.08455722033977509, G Loss : -0.18235959112644196\n",
      "Epoch[226/30000] - Batch : 1\n",
      "D Loss : -0.08711925148963928, G Loss : -0.1807701289653778\n",
      "Epoch[227/30000] - Batch : 1\n",
      "D Loss : -0.08694574236869812, G Loss : -0.1816977560520172\n",
      "Epoch[228/30000] - Batch : 1\n",
      "D Loss : -0.0879763662815094, G Loss : -0.1813080608844757\n",
      "Epoch[229/30000] - Batch : 1\n",
      "D Loss : -0.0884375274181366, G Loss : -0.18147709965705872\n",
      "Epoch[230/30000] - Batch : 1\n",
      "D Loss : -0.088721364736557, G Loss : -0.1818767488002777\n",
      "Epoch[231/30000] - Batch : 1\n",
      "D Loss : -0.08826807141304016, G Loss : -0.18296298384666443\n",
      "Epoch[232/30000] - Batch : 1\n",
      "D Loss : -0.08926962316036224, G Loss : -0.18256638944149017\n",
      "Epoch[233/30000] - Batch : 1\n",
      "D Loss : -0.09046143293380737, G Loss : -0.1819394826889038\n",
      "Epoch[234/30000] - Batch : 1\n",
      "D Loss : -0.09052634239196777, G Loss : -0.18243294954299927\n",
      "Epoch[235/30000] - Batch : 1\n",
      "D Loss : -0.09120851755142212, G Loss : -0.18226587772369385\n",
      "Epoch[236/30000] - Batch : 1\n",
      "D Loss : -0.09172457456588745, G Loss : -0.18227335810661316\n",
      "Epoch[237/30000] - Batch : 1\n",
      "D Loss : -0.09258347749710083, G Loss : -0.18192192912101746\n",
      "Epoch[238/30000] - Batch : 1\n",
      "D Loss : -0.09219971299171448, G Loss : -0.18277248740196228\n",
      "Epoch[239/30000] - Batch : 1\n",
      "D Loss : -0.09290260076522827, G Loss : -0.1825147569179535\n",
      "Epoch[240/30000] - Batch : 1\n",
      "D Loss : -0.09389570355415344, G Loss : -0.18195071816444397\n",
      "Epoch[241/30000] - Batch : 1\n",
      "D Loss : -0.09448446333408356, G Loss : -0.181766077876091\n",
      "Epoch[242/30000] - Batch : 1\n",
      "D Loss : -0.09503570199012756, G Loss : -0.181597501039505\n",
      "Epoch[243/30000] - Batch : 1\n",
      "D Loss : -0.09462630748748779, G Loss : -0.1823546588420868\n",
      "Epoch[244/30000] - Batch : 1\n",
      "D Loss : -0.09586140513420105, G Loss : -0.18144604563713074\n",
      "Epoch[245/30000] - Batch : 1\n",
      "D Loss : -0.096335768699646, G Loss : -0.18128246068954468\n",
      "Epoch[246/30000] - Batch : 1\n",
      "D Loss : -0.09675532579421997, G Loss : -0.18115538358688354\n",
      "Epoch[247/30000] - Batch : 1\n",
      "D Loss : -0.0974467396736145, G Loss : -0.18074065446853638\n",
      "Epoch[248/30000] - Batch : 1\n",
      "D Loss : -0.09765799343585968, G Loss : -0.18077902495861053\n",
      "Epoch[249/30000] - Batch : 1\n",
      "D Loss : -0.09840334951877594, G Loss : -0.18027694523334503\n",
      "Epoch[250/30000] - Batch : 1\n",
      "D Loss : -0.09882915019989014, G Loss : -0.18007725477218628\n",
      "Epoch[251/30000] - Batch : 1\n",
      "D Loss : -0.09914705157279968, G Loss : -0.17997363209724426\n",
      "Epoch[252/30000] - Batch : 1\n",
      "D Loss : -0.09946334362030029, G Loss : -0.17985528707504272\n",
      "Epoch[253/30000] - Batch : 1\n",
      "D Loss : -0.09999752044677734, G Loss : -0.17950543761253357\n",
      "Epoch[254/30000] - Batch : 1\n",
      "D Loss : -0.10046418011188507, G Loss : -0.17920704185962677\n",
      "Epoch[255/30000] - Batch : 1\n",
      "D Loss : -0.10086841881275177, G Loss : -0.17896346747875214\n",
      "Epoch[256/30000] - Batch : 1\n",
      "D Loss : -0.10125581920146942, G Loss : -0.1787264198064804\n",
      "Epoch[257/30000] - Batch : 1\n",
      "D Loss : -0.10165496170520782, G Loss : -0.17846937477588654\n",
      "Epoch[258/30000] - Batch : 1\n",
      "D Loss : -0.10202327370643616, G Loss : -0.17823576927185059\n",
      "Epoch[259/30000] - Batch : 1\n",
      "D Loss : -0.10238274931907654, G Loss : -0.1780048906803131\n",
      "Epoch[260/30000] - Batch : 1\n",
      "D Loss : -0.10283653438091278, G Loss : -0.17767532169818878\n",
      "Epoch[261/30000] - Batch : 1\n",
      "D Loss : -0.10320883989334106, G Loss : -0.17742305994033813\n",
      "Epoch[262/30000] - Batch : 1\n",
      "D Loss : -0.10386241972446442, G Loss : -0.1768520027399063\n",
      "Epoch[263/30000] - Batch : 1\n",
      "D Loss : -0.10398632287979126, G Loss : -0.17682814598083496\n",
      "Epoch[264/30000] - Batch : 1\n",
      "D Loss : -0.10439056158065796, G Loss : -0.1765250563621521\n",
      "Epoch[265/30000] - Batch : 1\n",
      "D Loss : -0.10472330451011658, G Loss : -0.17629459500312805\n",
      "Epoch[266/30000] - Batch : 1\n",
      "D Loss : -0.10508841276168823, G Loss : -0.17603835463523865\n",
      "Epoch[267/30000] - Batch : 1\n",
      "D Loss : -0.10560432076454163, G Loss : -0.17563417553901672\n",
      "Epoch[268/30000] - Batch : 1\n",
      "D Loss : -0.1060512512922287, G Loss : -0.17530448734760284\n",
      "Epoch[269/30000] - Batch : 1\n",
      "D Loss : -0.10620313882827759, G Loss : -0.17527341842651367\n",
      "Epoch[270/30000] - Batch : 1\n",
      "D Loss : -0.10677340626716614, G Loss : -0.17483314871788025\n",
      "Epoch[271/30000] - Batch : 1\n",
      "D Loss : -0.10714384913444519, G Loss : -0.17460212111473083\n",
      "Epoch[272/30000] - Batch : 1\n",
      "D Loss : -0.10750989615917206, G Loss : -0.1743953377008438\n",
      "Epoch[273/30000] - Batch : 1\n",
      "D Loss : -0.08874484896659851, G Loss : -0.19332417845726013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[274/30000] - Batch : 1\n",
      "D Loss : -0.10712999105453491, G Loss : -0.17513194680213928\n",
      "Epoch[275/30000] - Batch : 1\n",
      "D Loss : -0.10869626700878143, G Loss : -0.1738429218530655\n",
      "Epoch[276/30000] - Batch : 1\n",
      "D Loss : -0.10933510959148407, G Loss : -0.17366211116313934\n",
      "Epoch[277/30000] - Batch : 1\n",
      "D Loss : -0.10969701409339905, G Loss : -0.17351844906806946\n",
      "Epoch[278/30000] - Batch : 1\n",
      "D Loss : -0.10939060151576996, G Loss : -0.17400427162647247\n",
      "Epoch[279/30000] - Batch : 1\n",
      "D Loss : -0.11039102077484131, G Loss : -0.1731680929660797\n",
      "Epoch[280/30000] - Batch : 1\n",
      "D Loss : -0.11072327196598053, G Loss : -0.1729937344789505\n",
      "Epoch[281/30000] - Batch : 1\n",
      "D Loss : -0.10264983773231506, G Loss : -0.18116894364356995\n",
      "Epoch[282/30000] - Batch : 1\n",
      "D Loss : -0.11135557293891907, G Loss : -0.17259365320205688\n",
      "Epoch[283/30000] - Batch : 1\n",
      "D Loss : -0.10887756943702698, G Loss : -0.17512813210487366\n",
      "Epoch[284/30000] - Batch : 1\n",
      "D Loss : -0.11194616556167603, G Loss : -0.17215341329574585\n",
      "Epoch[285/30000] - Batch : 1\n",
      "D Loss : -0.11227017641067505, G Loss : -0.17193323373794556\n",
      "Epoch[286/30000] - Batch : 1\n",
      "D Loss : -0.11260034143924713, G Loss : -0.17171509563922882\n",
      "Epoch[287/30000] - Batch : 1\n",
      "D Loss : -0.11307880282402039, G Loss : -0.17135000228881836\n",
      "Epoch[288/30000] - Batch : 1\n",
      "D Loss : -0.11370982229709625, G Loss : -0.17082752287387848\n",
      "Epoch[289/30000] - Batch : 1\n",
      "D Loss : -0.11334560811519623, G Loss : -0.17098255455493927\n",
      "Epoch[290/30000] - Batch : 1\n",
      "D Loss : -0.1138959527015686, G Loss : -0.17061984539031982\n",
      "Epoch[291/30000] - Batch : 1\n",
      "D Loss : -0.11270582675933838, G Loss : -0.17181158065795898\n",
      "Epoch[292/30000] - Batch : 1\n",
      "D Loss : -0.11454619467258453, G Loss : -0.17003901302814484\n",
      "Epoch[293/30000] - Batch : 1\n",
      "D Loss : -0.1149103045463562, G Loss : -0.16979673504829407\n",
      "Epoch[294/30000] - Batch : 1\n",
      "D Loss : -0.11527025699615479, G Loss : -0.16957968473434448\n",
      "Epoch[295/30000] - Batch : 1\n",
      "D Loss : -0.11562404036521912, G Loss : -0.16938990354537964\n",
      "Epoch[296/30000] - Batch : 1\n",
      "D Loss : -0.11637789011001587, G Loss : -0.16883400082588196\n",
      "Epoch[297/30000] - Batch : 1\n",
      "D Loss : -0.11687543988227844, G Loss : -0.1685599982738495\n",
      "Epoch[298/30000] - Batch : 1\n",
      "D Loss : -0.11578148603439331, G Loss : -0.16990122199058533\n",
      "Epoch[299/30000] - Batch : 1\n",
      "D Loss : -0.11733278632164001, G Loss : -0.1686173677444458\n",
      "-0.99999726\n",
      "0.99999964\n",
      "0\n",
      "254\n",
      "D Real : [[0.53877443]\n",
      " [0.0336954 ]], D Fake : [[0.18477784]\n",
      " [0.1523352 ]]\n",
      "Epoch[300/30000] - Batch : 1\n",
      "D Loss : -0.11767840385437012, G Loss : -0.16855651140213013\n",
      "Epoch[301/30000] - Batch : 1\n",
      "D Loss : -0.11745321750640869, G Loss : -0.1690596640110016\n",
      "Epoch[302/30000] - Batch : 1\n",
      "D Loss : -0.069564089179039, G Loss : -0.2169709950685501\n",
      "Epoch[303/30000] - Batch : 1\n",
      "D Loss : -0.11646303534507751, G Loss : -0.16738370060920715\n",
      "Epoch[304/30000] - Batch : 1\n",
      "D Loss : -0.11700136959552765, G Loss : -0.1677493005990982\n",
      "Epoch[305/30000] - Batch : 1\n",
      "D Loss : -0.11576534807682037, G Loss : -0.16705100238323212\n",
      "Epoch[306/30000] - Batch : 1\n",
      "D Loss : -0.11584785580635071, G Loss : -0.16713732481002808\n",
      "Epoch[307/30000] - Batch : 1\n",
      "D Loss : -0.11146467924118042, G Loss : -0.17184409499168396\n",
      "Epoch[308/30000] - Batch : 1\n",
      "D Loss : -0.11478875577449799, G Loss : -0.16716594994068146\n",
      "Epoch[309/30000] - Batch : 1\n",
      "D Loss : -0.11604127287864685, G Loss : -0.16797155141830444\n",
      "Epoch[310/30000] - Batch : 1\n",
      "D Loss : -0.11550852656364441, G Loss : -0.16767019033432007\n",
      "Epoch[311/30000] - Batch : 1\n",
      "D Loss : -0.1179599016904831, G Loss : -0.1671363264322281\n",
      "Epoch[312/30000] - Batch : 1\n",
      "D Loss : -0.10694439709186554, G Loss : -0.17866216599941254\n",
      "Epoch[313/30000] - Batch : 1\n",
      "D Loss : -0.11911897361278534, G Loss : -0.16700966656208038\n",
      "Epoch[314/30000] - Batch : 1\n",
      "D Loss : -0.11860761046409607, G Loss : -0.1680460274219513\n",
      "Epoch[315/30000] - Batch : 1\n",
      "D Loss : -0.1167035698890686, G Loss : -0.16836056113243103\n",
      "Epoch[316/30000] - Batch : 1\n",
      "D Loss : -0.11814069747924805, G Loss : -0.16672983765602112\n",
      "Epoch[317/30000] - Batch : 1\n",
      "D Loss : -0.11964698135852814, G Loss : -0.167654350399971\n",
      "Epoch[318/30000] - Batch : 1\n",
      "D Loss : -0.12026971578598022, G Loss : -0.168129563331604\n",
      "Epoch[319/30000] - Batch : 1\n",
      "D Loss : -0.1215054988861084, G Loss : -0.16730868816375732\n",
      "Epoch[320/30000] - Batch : 1\n",
      "D Loss : -0.1115516722202301, G Loss : -0.1684580147266388\n",
      "Epoch[321/30000] - Batch : 1\n",
      "D Loss : -0.12039655447006226, G Loss : -0.16715314984321594\n",
      "Epoch[322/30000] - Batch : 1\n",
      "D Loss : -0.12042713165283203, G Loss : -0.16763639450073242\n",
      "Epoch[323/30000] - Batch : 1\n",
      "D Loss : -0.12131798267364502, G Loss : -0.16726765036582947\n",
      "Epoch[324/30000] - Batch : 1\n",
      "D Loss : -0.12437742948532104, G Loss : -0.16687539219856262\n",
      "Epoch[325/30000] - Batch : 1\n",
      "D Loss : -0.12205511331558228, G Loss : -0.1696232259273529\n",
      "Epoch[326/30000] - Batch : 1\n",
      "D Loss : -0.1237797886133194, G Loss : -0.16827641427516937\n",
      "Epoch[327/30000] - Batch : 1\n",
      "D Loss : -0.12584823369979858, G Loss : -0.166611909866333\n",
      "Epoch[328/30000] - Batch : 1\n",
      "D Loss : -0.12549005448818207, G Loss : -0.16731880605220795\n",
      "Epoch[329/30000] - Batch : 1\n",
      "D Loss : -0.11738552153110504, G Loss : -0.17569784820079803\n",
      "Epoch[330/30000] - Batch : 1\n",
      "D Loss : -0.12735052406787872, G Loss : -0.16613326966762543\n",
      "Epoch[331/30000] - Batch : 1\n",
      "D Loss : -0.1278569996356964, G Loss : -0.16596800088882446\n",
      "Epoch[332/30000] - Batch : 1\n",
      "D Loss : -0.126848503947258, G Loss : -0.16730235517024994\n",
      "Epoch[333/30000] - Batch : 1\n",
      "D Loss : -0.12396568059921265, G Loss : -0.1704549491405487\n",
      "Epoch[334/30000] - Batch : 1\n",
      "D Loss : -0.1292257010936737, G Loss : -0.16545885801315308\n",
      "Epoch[335/30000] - Batch : 1\n",
      "D Loss : -0.1270713061094284, G Loss : -0.16789673268795013\n",
      "Epoch[336/30000] - Batch : 1\n",
      "D Loss : -0.12673965096473694, G Loss : -0.1684620976448059\n",
      "Epoch[337/30000] - Batch : 1\n",
      "D Loss : -0.1254560649394989, G Loss : -0.16779422760009766\n",
      "Epoch[338/30000] - Batch : 1\n",
      "D Loss : -0.12582163512706757, G Loss : -0.1675708144903183\n",
      "Epoch[339/30000] - Batch : 1\n",
      "D Loss : -0.12679637968540192, G Loss : -0.1667221337556839\n",
      "Epoch[340/30000] - Batch : 1\n",
      "D Loss : -0.12889695167541504, G Loss : -0.1646563708782196\n",
      "Epoch[341/30000] - Batch : 1\n",
      "D Loss : -0.12872858345508575, G Loss : -0.1650618463754654\n",
      "Epoch[342/30000] - Batch : 1\n",
      "D Loss : -0.13004496693611145, G Loss : -0.1640397012233734\n",
      "Epoch[343/30000] - Batch : 1\n",
      "D Loss : -0.13146792352199554, G Loss : -0.16287387907505035\n",
      "Epoch[344/30000] - Batch : 1\n",
      "D Loss : -0.1297486424446106, G Loss : -0.1648024320602417\n",
      "Epoch[345/30000] - Batch : 1\n",
      "D Loss : -0.13169170916080475, G Loss : -0.16331784427165985\n",
      "Epoch[346/30000] - Batch : 1\n",
      "D Loss : -0.13331347703933716, G Loss : -0.16201195120811462\n",
      "Epoch[347/30000] - Batch : 1\n",
      "D Loss : -0.1332181841135025, G Loss : -0.16241450607776642\n",
      "Epoch[348/30000] - Batch : 1\n",
      "D Loss : -0.13383373618125916, G Loss : -0.16208899021148682\n",
      "Epoch[349/30000] - Batch : 1\n",
      "D Loss : -0.13221433758735657, G Loss : -0.16191142797470093\n",
      "Epoch[350/30000] - Batch : 1\n",
      "D Loss : -0.13487033545970917, G Loss : -0.1617307811975479\n",
      "Epoch[351/30000] - Batch : 1\n",
      "D Loss : -0.13484007120132446, G Loss : -0.16215738654136658\n",
      "Epoch[352/30000] - Batch : 1\n",
      "D Loss : -0.13520966470241547, G Loss : -0.16215451061725616\n",
      "Epoch[353/30000] - Batch : 1\n",
      "D Loss : -0.13561217486858368, G Loss : -0.16213463246822357\n",
      "Epoch[354/30000] - Batch : 1\n",
      "D Loss : -0.13650929927825928, G Loss : -0.16160348057746887\n",
      "Epoch[355/30000] - Batch : 1\n",
      "D Loss : -0.13551414012908936, G Loss : -0.1629924476146698\n",
      "Epoch[356/30000] - Batch : 1\n",
      "D Loss : -0.13727515935897827, G Loss : -0.1616380512714386\n",
      "Epoch[357/30000] - Batch : 1\n",
      "D Loss : -0.1377183496952057, G Loss : -0.16158482432365417\n",
      "Epoch[358/30000] - Batch : 1\n",
      "D Loss : -0.13837823271751404, G Loss : -0.16133612394332886\n",
      "Epoch[359/30000] - Batch : 1\n",
      "D Loss : -0.13842728734016418, G Loss : -0.16167166829109192\n",
      "Epoch[360/30000] - Batch : 1\n",
      "D Loss : -0.13860073685646057, G Loss : -0.16178679466247559\n",
      "Epoch[361/30000] - Batch : 1\n",
      "D Loss : -0.1396302729845047, G Loss : -0.16105984151363373\n",
      "Epoch[362/30000] - Batch : 1\n",
      "D Loss : -0.13965657353401184, G Loss : -0.1612531840801239\n",
      "Epoch[363/30000] - Batch : 1\n",
      "D Loss : -0.1404772400856018, G Loss : -0.16065159440040588\n",
      "Epoch[364/30000] - Batch : 1\n",
      "D Loss : -0.14041239023208618, G Loss : -0.16115418076515198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[365/30000] - Batch : 1\n",
      "D Loss : -0.1408068686723709, G Loss : -0.16104356944561005\n",
      "Epoch[366/30000] - Batch : 1\n",
      "D Loss : -0.1419193148612976, G Loss : -0.16020885109901428\n",
      "Epoch[367/30000] - Batch : 1\n",
      "D Loss : -0.14233753085136414, G Loss : -0.1600458323955536\n",
      "Epoch[368/30000] - Batch : 1\n",
      "D Loss : -0.14273804426193237, G Loss : -0.15991294384002686\n",
      "Epoch[369/30000] - Batch : 1\n",
      "D Loss : -0.14214327931404114, G Loss : -0.16070795059204102\n",
      "Epoch[370/30000] - Batch : 1\n",
      "D Loss : -0.1435094028711319, G Loss : -0.15952961146831512\n",
      "Epoch[371/30000] - Batch : 1\n",
      "D Loss : -0.14362239837646484, G Loss : -0.15962207317352295\n",
      "Epoch[372/30000] - Batch : 1\n",
      "D Loss : -0.1421346664428711, G Loss : -0.161260724067688\n",
      "Epoch[373/30000] - Batch : 1\n",
      "D Loss : -0.1427638828754425, G Loss : -0.16074120998382568\n",
      "Epoch[374/30000] - Batch : 1\n",
      "D Loss : -0.1450522243976593, G Loss : -0.15834471583366394\n",
      "Epoch[375/30000] - Batch : 1\n",
      "D Loss : -0.14520666003227234, G Loss : -0.15817612409591675\n",
      "Epoch[376/30000] - Batch : 1\n",
      "D Loss : -0.1455605924129486, G Loss : -0.15782630443572998\n",
      "Epoch[377/30000] - Batch : 1\n",
      "D Loss : -0.14445778727531433, G Loss : -0.15806645154953003\n",
      "Epoch[378/30000] - Batch : 1\n",
      "D Loss : -0.14629116654396057, G Loss : -0.15728843212127686\n",
      "Epoch[379/30000] - Batch : 1\n",
      "D Loss : -0.14541545510292053, G Loss : -0.1583016812801361\n",
      "Epoch[380/30000] - Batch : 1\n",
      "D Loss : -0.1470174789428711, G Loss : -0.15687373280525208\n",
      "Epoch[381/30000] - Batch : 1\n",
      "D Loss : -0.14732739329338074, G Loss : -0.15668389201164246\n",
      "Epoch[382/30000] - Batch : 1\n",
      "D Loss : -0.14853814244270325, G Loss : -0.15580150485038757\n",
      "Epoch[383/30000] - Batch : 1\n",
      "D Loss : -0.14821083843708038, G Loss : -0.15642310678958893\n",
      "Epoch[384/30000] - Batch : 1\n",
      "D Loss : -0.1482708901166916, G Loss : -0.15663845837116241\n",
      "Epoch[385/30000] - Batch : 1\n",
      "D Loss : -0.14837059378623962, G Loss : -0.15684774518013\n",
      "Epoch[386/30000] - Batch : 1\n",
      "D Loss : -0.1495186984539032, G Loss : -0.15622100234031677\n",
      "Epoch[387/30000] - Batch : 1\n",
      "D Loss : -0.15071187913417816, G Loss : -0.1561785191297531\n",
      "Epoch[388/30000] - Batch : 1\n",
      "D Loss : -0.15100741386413574, G Loss : -0.15612539649009705\n",
      "Epoch[389/30000] - Batch : 1\n",
      "D Loss : -0.15131062269210815, G Loss : -0.15617355704307556\n",
      "Epoch[390/30000] - Batch : 1\n",
      "D Loss : -0.15305447578430176, G Loss : -0.15480798482894897\n",
      "Epoch[391/30000] - Batch : 1\n",
      "D Loss : -0.15144793689250946, G Loss : -0.15673492848873138\n",
      "Epoch[392/30000] - Batch : 1\n",
      "D Loss : -0.15271420776844025, G Loss : -0.15577168762683868\n",
      "Epoch[393/30000] - Batch : 1\n",
      "D Loss : -0.15286245942115784, G Loss : -0.15592092275619507\n",
      "Epoch[394/30000] - Batch : 1\n",
      "D Loss : -0.1538296937942505, G Loss : -0.15525028109550476\n",
      "Epoch[395/30000] - Batch : 1\n",
      "D Loss : -0.15415771305561066, G Loss : -0.15520621836185455\n",
      "Epoch[396/30000] - Batch : 1\n",
      "D Loss : -0.15375706553459167, G Loss : -0.15563547611236572\n",
      "Epoch[397/30000] - Batch : 1\n",
      "D Loss : -0.15435439348220825, G Loss : -0.15558859705924988\n",
      "Epoch[398/30000] - Batch : 1\n",
      "D Loss : -0.15512584149837494, G Loss : -0.15506868064403534\n",
      "Epoch[399/30000] - Batch : 1\n",
      "D Loss : -0.1551142930984497, G Loss : -0.15530815720558167\n",
      "-0.9957028\n",
      "1.0\n",
      "0\n",
      "255\n",
      "D Real : [[0.599815 ]\n",
      " [0.0214738]], D Fake : [[0.12957618]\n",
      " [0.18096723]]\n",
      "Epoch[400/30000] - Batch : 1\n",
      "D Loss : -0.1553727090358734, G Loss : -0.1552717089653015\n",
      "Epoch[401/30000] - Batch : 1\n",
      "D Loss : -0.15568576753139496, G Loss : -0.15516062080860138\n",
      "Epoch[402/30000] - Batch : 1\n",
      "D Loss : -0.15756317973136902, G Loss : -0.15349557995796204\n",
      "Epoch[403/30000] - Batch : 1\n",
      "D Loss : -0.1564318686723709, G Loss : -0.15482179820537567\n",
      "Epoch[404/30000] - Batch : 1\n",
      "D Loss : -0.1570853292942047, G Loss : -0.1543441116809845\n",
      "Epoch[405/30000] - Batch : 1\n",
      "D Loss : -0.15527501702308655, G Loss : -0.15418162941932678\n",
      "Epoch[406/30000] - Batch : 1\n",
      "D Loss : -0.1387152224779129, G Loss : -0.17069615423679352\n",
      "Epoch[407/30000] - Batch : 1\n",
      "D Loss : -0.157822847366333, G Loss : -0.15369805693626404\n",
      "Epoch[408/30000] - Batch : 1\n",
      "D Loss : -0.15909454226493835, G Loss : -0.15239199995994568\n",
      "Epoch[409/30000] - Batch : 1\n",
      "D Loss : -0.1582871675491333, G Loss : -0.15319925546646118\n",
      "Epoch[410/30000] - Batch : 1\n",
      "D Loss : -0.15789151191711426, G Loss : -0.15362197160720825\n",
      "Epoch[411/30000] - Batch : 1\n",
      "D Loss : -0.1587754786014557, G Loss : -0.15279772877693176\n",
      "Epoch[412/30000] - Batch : 1\n",
      "D Loss : -0.1606440246105194, G Loss : -0.151035338640213\n",
      "Epoch[413/30000] - Batch : 1\n",
      "D Loss : -0.15883634984493256, G Loss : -0.15295527875423431\n",
      "Epoch[414/30000] - Batch : 1\n",
      "D Loss : -0.15980422496795654, G Loss : -0.15213361382484436\n",
      "Epoch[415/30000] - Batch : 1\n",
      "D Loss : -0.15928484499454498, G Loss : -0.15282677114009857\n",
      "Epoch[416/30000] - Batch : 1\n",
      "D Loss : -0.1594601422548294, G Loss : -0.15285180509090424\n",
      "Epoch[417/30000] - Batch : 1\n",
      "D Loss : -0.1610546112060547, G Loss : -0.1514940857887268\n",
      "Epoch[418/30000] - Batch : 1\n",
      "D Loss : -0.1612797975540161, G Loss : -0.1514657437801361\n",
      "Epoch[419/30000] - Batch : 1\n",
      "D Loss : -0.16030360758304596, G Loss : -0.15277869999408722\n",
      "Epoch[420/30000] - Batch : 1\n",
      "D Loss : -0.16220927238464355, G Loss : -0.15119072794914246\n",
      "Epoch[421/30000] - Batch : 1\n",
      "D Loss : -0.16199108958244324, G Loss : -0.15172573924064636\n",
      "Epoch[422/30000] - Batch : 1\n",
      "D Loss : -0.1620975285768509, G Loss : -0.15192945301532745\n",
      "Epoch[423/30000] - Batch : 1\n",
      "D Loss : -0.1627713441848755, G Loss : -0.15157315135002136\n",
      "Epoch[424/30000] - Batch : 1\n",
      "D Loss : -0.16216027736663818, G Loss : -0.15247690677642822\n",
      "Epoch[425/30000] - Batch : 1\n",
      "D Loss : -0.16130758821964264, G Loss : -0.1536097675561905\n",
      "Epoch[426/30000] - Batch : 1\n",
      "D Loss : -0.1638132780790329, G Loss : -0.15129776298999786\n",
      "Epoch[427/30000] - Batch : 1\n",
      "D Loss : -0.16477668285369873, G Loss : -0.1505187749862671\n",
      "Epoch[428/30000] - Batch : 1\n",
      "D Loss : -0.16422080993652344, G Loss : -0.15127167105674744\n",
      "Epoch[429/30000] - Batch : 1\n",
      "D Loss : -0.1651013344526291, G Loss : -0.15063537657260895\n",
      "Epoch[430/30000] - Batch : 1\n",
      "D Loss : -0.16477590799331665, G Loss : -0.1511721909046173\n",
      "Epoch[431/30000] - Batch : 1\n",
      "D Loss : -0.16542106866836548, G Loss : -0.15074661374092102\n",
      "Epoch[432/30000] - Batch : 1\n",
      "D Loss : -0.1662936508655548, G Loss : -0.15010324120521545\n",
      "Epoch[433/30000] - Batch : 1\n",
      "D Loss : -0.16700509190559387, G Loss : -0.1496278941631317\n",
      "Epoch[434/30000] - Batch : 1\n",
      "D Loss : -0.1673797369003296, G Loss : -0.149512380361557\n",
      "Epoch[435/30000] - Batch : 1\n",
      "D Loss : -0.16389307379722595, G Loss : -0.15297868847846985\n",
      "Epoch[436/30000] - Batch : 1\n",
      "D Loss : -0.16553613543510437, G Loss : -0.15170323848724365\n",
      "Epoch[437/30000] - Batch : 1\n",
      "D Loss : -0.16478288173675537, G Loss : -0.1506134271621704\n",
      "Epoch[438/30000] - Batch : 1\n",
      "D Loss : -0.16432660818099976, G Loss : -0.14917859435081482\n",
      "Epoch[439/30000] - Batch : 1\n",
      "D Loss : -0.16678480803966522, G Loss : -0.14891968667507172\n",
      "Epoch[440/30000] - Batch : 1\n",
      "D Loss : -0.1660173237323761, G Loss : -0.14993217587471008\n",
      "Epoch[441/30000] - Batch : 1\n",
      "D Loss : -0.16597530245780945, G Loss : -0.15022501349449158\n",
      "Epoch[442/30000] - Batch : 1\n",
      "D Loss : -0.16626285016536713, G Loss : -0.1498316079378128\n",
      "Epoch[443/30000] - Batch : 1\n",
      "D Loss : -0.16638609766960144, G Loss : -0.15036842226982117\n",
      "Epoch[444/30000] - Batch : 1\n",
      "D Loss : -0.16834679245948792, G Loss : -0.14873138070106506\n",
      "Epoch[445/30000] - Batch : 1\n",
      "D Loss : -0.1675851047039032, G Loss : -0.1497807800769806\n",
      "Epoch[446/30000] - Batch : 1\n",
      "D Loss : -0.1679459512233734, G Loss : -0.14970126748085022\n",
      "Epoch[447/30000] - Batch : 1\n",
      "D Loss : -0.16876471042633057, G Loss : -0.14914843440055847\n",
      "Epoch[448/30000] - Batch : 1\n",
      "D Loss : -0.16848528385162354, G Loss : -0.1496574878692627\n",
      "Epoch[449/30000] - Batch : 1\n",
      "D Loss : -0.16956868767738342, G Loss : -0.14891499280929565\n",
      "Epoch[450/30000] - Batch : 1\n",
      "D Loss : -0.1690138280391693, G Loss : -0.14951875805854797\n",
      "Epoch[451/30000] - Batch : 1\n",
      "D Loss : -0.17028993368148804, G Loss : -0.14875870943069458\n",
      "Epoch[452/30000] - Batch : 1\n",
      "D Loss : -0.1706741750240326, G Loss : -0.1486242413520813\n",
      "Epoch[453/30000] - Batch : 1\n",
      "D Loss : -0.17094480991363525, G Loss : -0.14856061339378357\n",
      "Epoch[454/30000] - Batch : 1\n",
      "D Loss : -0.17122553288936615, G Loss : -0.1485363095998764\n",
      "Epoch[455/30000] - Batch : 1\n",
      "D Loss : -0.17135754227638245, G Loss : -0.14861539006233215\n",
      "Epoch[456/30000] - Batch : 1\n",
      "D Loss : -0.17162808775901794, G Loss : -0.14853525161743164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[457/30000] - Batch : 1\n",
      "D Loss : -0.17058065533638, G Loss : -0.14973437786102295\n",
      "Epoch[458/30000] - Batch : 1\n",
      "D Loss : -0.17053759098052979, G Loss : -0.14990079402923584\n",
      "Epoch[459/30000] - Batch : 1\n",
      "D Loss : -0.17226460576057434, G Loss : -0.14808380603790283\n",
      "Epoch[460/30000] - Batch : 1\n",
      "D Loss : -0.16979925334453583, G Loss : -0.14811842143535614\n",
      "Epoch[461/30000] - Batch : 1\n",
      "D Loss : -0.17121663689613342, G Loss : -0.14882060885429382\n",
      "Epoch[462/30000] - Batch : 1\n",
      "D Loss : -0.16843487322330475, G Loss : -0.14739398658275604\n",
      "Epoch[463/30000] - Batch : 1\n",
      "D Loss : -0.17016282677650452, G Loss : -0.14720749855041504\n",
      "Epoch[464/30000] - Batch : 1\n",
      "D Loss : -0.1710835099220276, G Loss : -0.14715349674224854\n",
      "Epoch[465/30000] - Batch : 1\n",
      "D Loss : -0.17144563794136047, G Loss : -0.14705708622932434\n",
      "Epoch[466/30000] - Batch : 1\n",
      "D Loss : -0.17170864343643188, G Loss : -0.14700797200202942\n",
      "Epoch[467/30000] - Batch : 1\n",
      "D Loss : -0.17185772955417633, G Loss : -0.14704282581806183\n",
      "Epoch[468/30000] - Batch : 1\n",
      "D Loss : -0.17218047380447388, G Loss : -0.14710867404937744\n",
      "Epoch[469/30000] - Batch : 1\n",
      "D Loss : -0.17076599597930908, G Loss : -0.14871561527252197\n",
      "Epoch[470/30000] - Batch : 1\n",
      "D Loss : -0.17198175191879272, G Loss : -0.14799940586090088\n",
      "Epoch[471/30000] - Batch : 1\n",
      "D Loss : -0.17307136952877045, G Loss : -0.14726440608501434\n",
      "Epoch[472/30000] - Batch : 1\n",
      "D Loss : -0.17280811071395874, G Loss : -0.1478484869003296\n",
      "Epoch[473/30000] - Batch : 1\n",
      "D Loss : -0.1735597550868988, G Loss : -0.14742645621299744\n",
      "Epoch[474/30000] - Batch : 1\n",
      "D Loss : -0.16124770045280457, G Loss : -0.16006067395210266\n",
      "Epoch[475/30000] - Batch : 1\n",
      "D Loss : -0.1741017997264862, G Loss : -0.1474628746509552\n",
      "Epoch[476/30000] - Batch : 1\n",
      "D Loss : -0.17405471205711365, G Loss : -0.14780649542808533\n",
      "Epoch[477/30000] - Batch : 1\n",
      "D Loss : -0.17345723509788513, G Loss : -0.1482258141040802\n",
      "Epoch[478/30000] - Batch : 1\n",
      "D Loss : -0.17497430741786957, G Loss : -0.14728276431560516\n",
      "Epoch[479/30000] - Batch : 1\n",
      "D Loss : -0.17415794730186462, G Loss : -0.14720755815505981\n",
      "Epoch[480/30000] - Batch : 1\n",
      "D Loss : -0.173346608877182, G Loss : -0.14710015058517456\n",
      "Epoch[481/30000] - Batch : 1\n",
      "D Loss : -0.17358306050300598, G Loss : -0.14703965187072754\n",
      "Epoch[482/30000] - Batch : 1\n",
      "D Loss : -0.1729031801223755, G Loss : -0.14790883660316467\n",
      "Epoch[483/30000] - Batch : 1\n",
      "D Loss : -0.17400699853897095, G Loss : -0.147024005651474\n",
      "Epoch[484/30000] - Batch : 1\n",
      "D Loss : -0.17432740330696106, G Loss : -0.1469162404537201\n",
      "Epoch[485/30000] - Batch : 1\n",
      "D Loss : -0.17382881045341492, G Loss : -0.1476278305053711\n",
      "Epoch[486/30000] - Batch : 1\n",
      "D Loss : -0.17482352256774902, G Loss : -0.1468568742275238\n",
      "Epoch[487/30000] - Batch : 1\n",
      "D Loss : -0.17507240176200867, G Loss : -0.14683622121810913\n",
      "Epoch[488/30000] - Batch : 1\n",
      "D Loss : -0.1752992868423462, G Loss : -0.14681485295295715\n",
      "Epoch[489/30000] - Batch : 1\n",
      "D Loss : -0.17486761510372162, G Loss : -0.14743097126483917\n",
      "Epoch[490/30000] - Batch : 1\n",
      "D Loss : -0.17513230443000793, G Loss : -0.147342711687088\n",
      "Epoch[491/30000] - Batch : 1\n",
      "D Loss : -0.17480206489562988, G Loss : -0.14781713485717773\n",
      "Epoch[492/30000] - Batch : 1\n",
      "D Loss : -0.17614197731018066, G Loss : -0.14663395285606384\n",
      "Epoch[493/30000] - Batch : 1\n",
      "D Loss : -0.17618919909000397, G Loss : -0.1467227190732956\n",
      "Epoch[494/30000] - Batch : 1\n",
      "D Loss : -0.1765325367450714, G Loss : -0.14651033282279968\n",
      "Epoch[495/30000] - Batch : 1\n",
      "D Loss : -0.1767025589942932, G Loss : -0.14645591378211975\n",
      "Epoch[496/30000] - Batch : 1\n",
      "D Loss : -0.1768818199634552, G Loss : -0.1463891863822937\n",
      "Epoch[497/30000] - Batch : 1\n",
      "D Loss : -0.17706671357154846, G Loss : -0.14631015062332153\n",
      "Epoch[498/30000] - Batch : 1\n",
      "D Loss : -0.17725086212158203, G Loss : -0.14622771739959717\n",
      "Epoch[499/30000] - Batch : 1\n",
      "D Loss : -0.17700433731079102, G Loss : -0.14657562971115112\n",
      "-0.9999977\n",
      "1.0000001\n",
      "0\n",
      "255\n",
      "D Real : [[0.01685282]\n",
      " [0.6304442 ]], D Fake : [[0.13410637]\n",
      " [0.15882604]]\n",
      "Epoch[500/30000] - Batch : 1\n",
      "D Loss : -0.17718231678009033, G Loss : -0.1464661955833435\n",
      "Epoch[501/30000] - Batch : 1\n",
      "D Loss : -0.17778030037879944, G Loss : -0.1459517776966095\n",
      "Epoch[502/30000] - Batch : 1\n",
      "D Loss : -0.17788761854171753, G Loss : -0.1459311842918396\n",
      "Epoch[503/30000] - Batch : 1\n",
      "D Loss : -0.17797181010246277, G Loss : -0.14593958854675293\n",
      "Epoch[504/30000] - Batch : 1\n",
      "D Loss : -0.17838813364505768, G Loss : -0.1456233114004135\n",
      "Epoch[505/30000] - Batch : 1\n",
      "D Loss : -0.17858293652534485, G Loss : -0.14553788304328918\n",
      "Epoch[506/30000] - Batch : 1\n",
      "D Loss : -0.1787145584821701, G Loss : -0.14552341401576996\n",
      "Epoch[507/30000] - Batch : 1\n",
      "D Loss : -0.17876577377319336, G Loss : -0.14559528231620789\n",
      "Epoch[508/30000] - Batch : 1\n",
      "D Loss : -0.17923274636268616, G Loss : -0.14525893330574036\n",
      "Epoch[509/30000] - Batch : 1\n",
      "D Loss : -0.1795140653848648, G Loss : -0.14511601626873016\n",
      "Epoch[510/30000] - Batch : 1\n",
      "D Loss : -0.17962367832660675, G Loss : -0.14515243470668793\n",
      "Epoch[511/30000] - Batch : 1\n",
      "D Loss : -0.1799953579902649, G Loss : -0.1449359655380249\n",
      "Epoch[512/30000] - Batch : 1\n",
      "D Loss : -0.1802223026752472, G Loss : -0.14486989378929138\n",
      "Epoch[513/30000] - Batch : 1\n",
      "D Loss : -0.18047568202018738, G Loss : -0.1447840929031372\n",
      "Epoch[514/30000] - Batch : 1\n",
      "D Loss : -0.18067848682403564, G Loss : -0.14475518465042114\n",
      "Epoch[515/30000] - Batch : 1\n",
      "D Loss : -0.18096868693828583, G Loss : -0.14464090764522552\n",
      "Epoch[516/30000] - Batch : 1\n",
      "D Loss : -0.18129661679267883, G Loss : -0.1444949507713318\n",
      "Epoch[517/30000] - Batch : 1\n",
      "D Loss : -0.1811462938785553, G Loss : -0.14482218027114868\n",
      "Epoch[518/30000] - Batch : 1\n",
      "D Loss : -0.18185248970985413, G Loss : -0.14430108666419983\n",
      "Epoch[519/30000] - Batch : 1\n",
      "D Loss : -0.1821441650390625, G Loss : -0.14419591426849365\n",
      "Epoch[520/30000] - Batch : 1\n",
      "D Loss : -0.18240313231945038, G Loss : -0.14410866796970367\n",
      "Epoch[521/30000] - Batch : 1\n",
      "D Loss : -0.18267080187797546, G Loss : -0.14401665329933167\n",
      "Epoch[522/30000] - Batch : 1\n",
      "D Loss : -0.1823335587978363, G Loss : -0.1445169448852539\n",
      "Epoch[523/30000] - Batch : 1\n",
      "D Loss : -0.18322843313217163, G Loss : -0.14379388093948364\n",
      "Epoch[524/30000] - Batch : 1\n",
      "D Loss : -0.18349826335906982, G Loss : -0.14368155598640442\n",
      "Epoch[525/30000] - Batch : 1\n",
      "D Loss : -0.1837647408246994, G Loss : -0.1435726433992386\n",
      "Epoch[526/30000] - Batch : 1\n",
      "D Loss : -0.18398262560367584, G Loss : -0.14351733028888702\n",
      "Epoch[527/30000] - Batch : 1\n",
      "D Loss : -0.18430623412132263, G Loss : -0.14335897564888\n",
      "Epoch[528/30000] - Batch : 1\n",
      "D Loss : -0.15297344326972961, G Loss : -0.17485475540161133\n",
      "Epoch[529/30000] - Batch : 1\n",
      "D Loss : -0.1842687726020813, G Loss : -0.14366549253463745\n",
      "Epoch[530/30000] - Batch : 1\n",
      "D Loss : -0.18510329723358154, G Loss : -0.1429360806941986\n",
      "Epoch[531/30000] - Batch : 1\n",
      "D Loss : -0.1852971464395523, G Loss : -0.14285020530223846\n",
      "Epoch[532/30000] - Batch : 1\n",
      "D Loss : -0.18364623188972473, G Loss : -0.142683744430542\n",
      "Epoch[533/30000] - Batch : 1\n",
      "D Loss : -0.18444547057151794, G Loss : -0.1426989734172821\n",
      "Epoch[534/30000] - Batch : 1\n",
      "D Loss : -0.18611161410808563, G Loss : -0.14255143702030182\n",
      "Epoch[535/30000] - Batch : 1\n",
      "D Loss : -0.1857871115207672, G Loss : -0.14307290315628052\n",
      "Epoch[536/30000] - Batch : 1\n",
      "D Loss : -0.18730495870113373, G Loss : -0.14240165054798126\n",
      "Epoch[537/30000] - Batch : 1\n",
      "D Loss : -0.18746279180049896, G Loss : -0.1424575001001358\n",
      "Epoch[538/30000] - Batch : 1\n",
      "D Loss : -0.18657848238945007, G Loss : -0.1434648334980011\n",
      "Epoch[539/30000] - Batch : 1\n",
      "D Loss : -0.18645009398460388, G Loss : -0.14368754625320435\n",
      "Epoch[540/30000] - Batch : 1\n",
      "D Loss : -0.1881064772605896, G Loss : -0.14212048053741455\n",
      "Epoch[541/30000] - Batch : 1\n",
      "D Loss : -0.18846139311790466, G Loss : -0.14185696840286255\n",
      "Epoch[542/30000] - Batch : 1\n",
      "D Loss : -0.1884506642818451, G Loss : -0.1419367790222168\n",
      "Epoch[543/30000] - Batch : 1\n",
      "D Loss : -0.18886131048202515, G Loss : -0.14159679412841797\n",
      "Epoch[544/30000] - Batch : 1\n",
      "D Loss : -0.18301397562026978, G Loss : -0.147504985332489\n",
      "Epoch[545/30000] - Batch : 1\n",
      "D Loss : -0.18923884630203247, G Loss : -0.1413426697254181\n",
      "Epoch[546/30000] - Batch : 1\n",
      "D Loss : -0.18942278623580933, G Loss : -0.1412336230278015\n",
      "Epoch[547/30000] - Batch : 1\n",
      "D Loss : -0.18963134288787842, G Loss : -0.1411098837852478\n",
      "Epoch[548/30000] - Batch : 1\n",
      "D Loss : -0.1898360401391983, G Loss : -0.1410013884305954\n",
      "Epoch[549/30000] - Batch : 1\n",
      "D Loss : -0.18998868763446808, G Loss : -0.14095382392406464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[550/30000] - Batch : 1\n",
      "D Loss : -0.19026178121566772, G Loss : -0.14079952239990234\n",
      "Epoch[551/30000] - Batch : 1\n",
      "D Loss : -0.18940144777297974, G Loss : -0.1417650580406189\n",
      "Epoch[552/30000] - Batch : 1\n",
      "D Loss : -0.19070592522621155, G Loss : -0.1405881643295288\n",
      "Epoch[553/30000] - Batch : 1\n",
      "D Loss : -0.19087311625480652, G Loss : -0.14055991172790527\n",
      "Epoch[554/30000] - Batch : 1\n",
      "D Loss : -0.19075769186019897, G Loss : -0.1408112347126007\n",
      "Epoch[555/30000] - Batch : 1\n",
      "D Loss : -0.191420316696167, G Loss : -0.14030390977859497\n",
      "Epoch[556/30000] - Batch : 1\n",
      "D Loss : -0.19159787893295288, G Loss : -0.1402912139892578\n",
      "Epoch[557/30000] - Batch : 1\n",
      "D Loss : -0.19192951917648315, G Loss : -0.1401372253894806\n",
      "Epoch[558/30000] - Batch : 1\n",
      "D Loss : -0.19219636917114258, G Loss : -0.14005059003829956\n",
      "Epoch[559/30000] - Batch : 1\n",
      "D Loss : -0.19248148798942566, G Loss : -0.1399613916873932\n",
      "Epoch[560/30000] - Batch : 1\n",
      "D Loss : -0.1927439272403717, G Loss : -0.1399027705192566\n",
      "Epoch[561/30000] - Batch : 1\n",
      "D Loss : -0.19291368126869202, G Loss : -0.13993912935256958\n",
      "Epoch[562/30000] - Batch : 1\n",
      "D Loss : -0.19320796430110931, G Loss : -0.13985948264598846\n",
      "Epoch[563/30000] - Batch : 1\n",
      "D Loss : -0.19276612997055054, G Loss : -0.1404826045036316\n",
      "Epoch[564/30000] - Batch : 1\n",
      "D Loss : -0.19387832283973694, G Loss : -0.13956671953201294\n",
      "Epoch[565/30000] - Batch : 1\n",
      "D Loss : -0.1941559910774231, G Loss : -0.13947415351867676\n",
      "Epoch[566/30000] - Batch : 1\n",
      "D Loss : -0.19456443190574646, G Loss : -0.13923251628875732\n",
      "Epoch[567/30000] - Batch : 1\n",
      "D Loss : -0.1947116106748581, G Loss : -0.1392548829317093\n",
      "Epoch[568/30000] - Batch : 1\n",
      "D Loss : -0.19512265920639038, G Loss : -0.139020174741745\n",
      "Epoch[569/30000] - Batch : 1\n",
      "D Loss : -0.19530197978019714, G Loss : -0.13901501893997192\n",
      "Epoch[570/30000] - Batch : 1\n",
      "D Loss : -0.19558650255203247, G Loss : -0.13886502385139465\n",
      "Epoch[571/30000] - Batch : 1\n",
      "D Loss : -0.19572651386260986, G Loss : -0.13885223865509033\n",
      "Epoch[572/30000] - Batch : 1\n",
      "D Loss : -0.19590350985527039, G Loss : -0.13879117369651794\n",
      "Epoch[573/30000] - Batch : 1\n",
      "D Loss : -0.19615665078163147, G Loss : -0.138657808303833\n",
      "Epoch[574/30000] - Batch : 1\n",
      "D Loss : -0.1963416486978531, G Loss : -0.13859446346759796\n",
      "Epoch[575/30000] - Batch : 1\n",
      "D Loss : -0.19652393460273743, G Loss : -0.13853466510772705\n",
      "Epoch[576/30000] - Batch : 1\n",
      "D Loss : -0.1967180073261261, G Loss : -0.1384637951850891\n",
      "Epoch[577/30000] - Batch : 1\n",
      "D Loss : -0.1968384087085724, G Loss : -0.13846829533576965\n",
      "Epoch[578/30000] - Batch : 1\n",
      "D Loss : -0.19712421298027039, G Loss : -0.13831114768981934\n",
      "Epoch[579/30000] - Batch : 1\n",
      "D Loss : -0.197344571352005, G Loss : -0.13817694783210754\n",
      "Epoch[580/30000] - Batch : 1\n",
      "D Loss : -0.19749735295772552, G Loss : -0.13810496032238007\n",
      "Epoch[581/30000] - Batch : 1\n",
      "D Loss : -0.19766460359096527, G Loss : -0.13801313936710358\n",
      "Epoch[582/30000] - Batch : 1\n",
      "D Loss : -0.1976408064365387, G Loss : -0.13811463117599487\n",
      "Epoch[583/30000] - Batch : 1\n",
      "D Loss : -0.19788482785224915, G Loss : -0.13787567615509033\n",
      "Epoch[584/30000] - Batch : 1\n",
      "D Loss : -0.19760611653327942, G Loss : -0.13830122351646423\n",
      "Epoch[585/30000] - Batch : 1\n",
      "D Loss : -0.1980505734682083, G Loss : -0.13777320086956024\n",
      "Epoch[586/30000] - Batch : 1\n",
      "D Loss : -0.19842781126499176, G Loss : -0.13760365545749664\n",
      "Epoch[587/30000] - Batch : 1\n",
      "D Loss : -0.19858309626579285, G Loss : -0.13763102889060974\n",
      "Epoch[588/30000] - Batch : 1\n",
      "D Loss : -0.19881050288677216, G Loss : -0.13748525083065033\n",
      "Epoch[589/30000] - Batch : 1\n",
      "D Loss : -0.19894208014011383, G Loss : -0.13746590912342072\n",
      "Epoch[590/30000] - Batch : 1\n",
      "D Loss : -0.1991502344608307, G Loss : -0.13737523555755615\n",
      "Epoch[591/30000] - Batch : 1\n",
      "D Loss : -0.19843071699142456, G Loss : -0.1382143497467041\n",
      "Epoch[592/30000] - Batch : 1\n",
      "D Loss : -0.1986716240644455, G Loss : -0.13809533417224884\n",
      "Epoch[593/30000] - Batch : 1\n",
      "D Loss : -0.1995602250099182, G Loss : -0.13731753826141357\n",
      "Epoch[594/30000] - Batch : 1\n",
      "D Loss : -0.19843363761901855, G Loss : -0.13850799202919006\n",
      "Epoch[595/30000] - Batch : 1\n",
      "D Loss : -0.19215886294841766, G Loss : -0.14486603438854218\n",
      "Epoch[596/30000] - Batch : 1\n",
      "D Loss : -0.19732242822647095, G Loss : -0.13844826817512512\n",
      "Epoch[597/30000] - Batch : 1\n",
      "D Loss : -0.19992747902870178, G Loss : -0.13698339462280273\n",
      "Epoch[598/30000] - Batch : 1\n",
      "D Loss : -0.19900251924991608, G Loss : -0.13662733137607574\n",
      "Epoch[599/30000] - Batch : 1\n",
      "D Loss : -0.1993519812822342, G Loss : -0.1370280534029007\n",
      "-0.9999647\n",
      "0.9999999\n",
      "0\n",
      "254\n",
      "D Real : [[0.01286835]\n",
      " [0.65709966]], D Fake : [[0.13920267]\n",
      " [0.13380706]]\n",
      "Epoch[600/30000] - Batch : 1\n",
      "D Loss : -0.19847914576530457, G Loss : -0.1365048587322235\n",
      "Epoch[601/30000] - Batch : 1\n",
      "D Loss : -0.198221355676651, G Loss : -0.13693350553512573\n",
      "Epoch[602/30000] - Batch : 1\n",
      "D Loss : -0.20073753595352173, G Loss : -0.13683578372001648\n",
      "Epoch[603/30000] - Batch : 1\n",
      "D Loss : -0.20107266306877136, G Loss : -0.13674351572990417\n",
      "Epoch[604/30000] - Batch : 1\n",
      "D Loss : -0.20137307047843933, G Loss : -0.1366616189479828\n",
      "Epoch[605/30000] - Batch : 1\n",
      "D Loss : -0.20134279131889343, G Loss : -0.13689497113227844\n",
      "Epoch[606/30000] - Batch : 1\n",
      "D Loss : -0.20182214677333832, G Loss : -0.13658522069454193\n",
      "Epoch[607/30000] - Batch : 1\n",
      "D Loss : -0.2019001543521881, G Loss : -0.13665124773979187\n",
      "Epoch[608/30000] - Batch : 1\n",
      "D Loss : -0.2021874487400055, G Loss : -0.13648059964179993\n",
      "Epoch[609/30000] - Batch : 1\n",
      "D Loss : -0.20201675593852997, G Loss : -0.136727437376976\n",
      "Epoch[610/30000] - Batch : 1\n",
      "D Loss : -0.20223116874694824, G Loss : -0.13658234477043152\n",
      "Epoch[611/30000] - Batch : 1\n",
      "D Loss : -0.20257899165153503, G Loss : -0.13633224368095398\n",
      "Epoch[612/30000] - Batch : 1\n",
      "D Loss : -0.20268496870994568, G Loss : -0.13629364967346191\n",
      "Epoch[613/30000] - Batch : 1\n",
      "D Loss : -0.20285260677337646, G Loss : -0.13618186116218567\n",
      "Epoch[614/30000] - Batch : 1\n",
      "D Loss : -0.12997554242610931, G Loss : -0.20791910588741302\n",
      "Epoch[615/30000] - Batch : 1\n",
      "D Loss : -0.20276960730552673, G Loss : -0.13439610600471497\n",
      "Epoch[616/30000] - Batch : 1\n",
      "D Loss : -0.20233182609081268, G Loss : -0.13424251973628998\n",
      "Epoch[617/30000] - Batch : 1\n",
      "D Loss : -0.20298944413661957, G Loss : -0.13316519558429718\n",
      "Epoch[618/30000] - Batch : 1\n",
      "D Loss : -0.20307986438274384, G Loss : -0.13281549513339996\n",
      "Epoch[619/30000] - Batch : 1\n",
      "D Loss : -0.2033480405807495, G Loss : -0.13247862458229065\n",
      "Epoch[620/30000] - Batch : 1\n",
      "D Loss : -0.20349149405956268, G Loss : -0.1324400156736374\n",
      "Epoch[621/30000] - Batch : 1\n",
      "D Loss : -0.2032518982887268, G Loss : -0.1329314112663269\n",
      "Epoch[622/30000] - Batch : 1\n",
      "D Loss : -0.20379257202148438, G Loss : -0.13277649879455566\n",
      "Epoch[623/30000] - Batch : 1\n",
      "D Loss : -0.20393170416355133, G Loss : -0.13311649858951569\n",
      "Epoch[624/30000] - Batch : 1\n",
      "D Loss : -0.20402836799621582, G Loss : -0.1335609257221222\n",
      "Epoch[625/30000] - Batch : 1\n",
      "D Loss : -0.20413079857826233, G Loss : -0.13403740525245667\n",
      "Epoch[626/30000] - Batch : 1\n",
      "D Loss : -0.2014724612236023, G Loss : -0.1372743844985962\n",
      "Epoch[627/30000] - Batch : 1\n",
      "D Loss : -0.20428425073623657, G Loss : -0.13503286242485046\n",
      "Epoch[628/30000] - Batch : 1\n",
      "D Loss : -0.20425377786159515, G Loss : -0.13559778034687042\n",
      "Epoch[629/30000] - Batch : 1\n",
      "D Loss : -0.2044879049062729, G Loss : -0.13584278523921967\n",
      "Epoch[630/30000] - Batch : 1\n",
      "D Loss : -0.1937311887741089, G Loss : -0.14689946174621582\n",
      "Epoch[631/30000] - Batch : 1\n",
      "D Loss : -0.2046487033367157, G Loss : -0.13621366024017334\n",
      "Epoch[632/30000] - Batch : 1\n",
      "D Loss : -0.20444926619529724, G Loss : -0.13657015562057495\n",
      "Epoch[633/30000] - Batch : 1\n",
      "D Loss : -0.2047216147184372, G Loss : -0.13639043271541595\n",
      "Epoch[634/30000] - Batch : 1\n",
      "D Loss : -0.20477330684661865, G Loss : -0.13636699318885803\n",
      "Epoch[635/30000] - Batch : 1\n",
      "D Loss : -0.2049177587032318, G Loss : -0.13620266318321228\n",
      "Epoch[636/30000] - Batch : 1\n",
      "D Loss : -0.20497871935367584, G Loss : -0.13607852160930634\n",
      "Epoch[637/30000] - Batch : 1\n",
      "D Loss : -0.20505748689174652, G Loss : -0.13590724766254425\n",
      "Epoch[638/30000] - Batch : 1\n",
      "D Loss : -0.20500119030475616, G Loss : -0.13583387434482574\n",
      "Epoch[639/30000] - Batch : 1\n",
      "D Loss : -0.2052077054977417, G Loss : -0.13549169898033142\n",
      "Epoch[640/30000] - Batch : 1\n",
      "D Loss : -0.20527684688568115, G Loss : -0.1352844536304474\n",
      "Epoch[641/30000] - Batch : 1\n",
      "D Loss : -0.205290287733078, G Loss : -0.1351383626461029\n",
      "Epoch[642/30000] - Batch : 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D Loss : -0.20543235540390015, G Loss : -0.1348777413368225\n",
      "Epoch[643/30000] - Batch : 1\n",
      "D Loss : -0.20536071062088013, G Loss : -0.13415482640266418\n",
      "Epoch[644/30000] - Batch : 1\n",
      "D Loss : -0.20541009306907654, G Loss : -0.1335204839706421\n",
      "Epoch[645/30000] - Batch : 1\n",
      "D Loss : -0.2055051028728485, G Loss : -0.13298162817955017\n",
      "Epoch[646/30000] - Batch : 1\n",
      "D Loss : -0.20561984181404114, G Loss : -0.13257113099098206\n",
      "Epoch[647/30000] - Batch : 1\n",
      "D Loss : -0.20573323965072632, G Loss : -0.13231247663497925\n",
      "Epoch[648/30000] - Batch : 1\n",
      "D Loss : -0.20551973581314087, G Loss : -0.13251444697380066\n",
      "Epoch[649/30000] - Batch : 1\n",
      "D Loss : -0.2057754397392273, G Loss : -0.13236308097839355\n",
      "Epoch[650/30000] - Batch : 1\n",
      "D Loss : -0.2059343457221985, G Loss : -0.13195112347602844\n",
      "Epoch[651/30000] - Batch : 1\n",
      "D Loss : -0.20598003268241882, G Loss : -0.1318553388118744\n",
      "Epoch[652/30000] - Batch : 1\n",
      "D Loss : -0.2059263288974762, G Loss : -0.13199594616889954\n",
      "Epoch[653/30000] - Batch : 1\n",
      "D Loss : -0.20601174235343933, G Loss : -0.1321086585521698\n",
      "Epoch[654/30000] - Batch : 1\n",
      "D Loss : -0.2042495310306549, G Loss : -0.13419628143310547\n",
      "Epoch[655/30000] - Batch : 1\n",
      "D Loss : -0.2054506540298462, G Loss : -0.13334640860557556\n",
      "Epoch[656/30000] - Batch : 1\n",
      "D Loss : -0.20530420541763306, G Loss : -0.13381323218345642\n",
      "Epoch[657/30000] - Batch : 1\n",
      "D Loss : -0.20597527921199799, G Loss : -0.13350005447864532\n",
      "Epoch[658/30000] - Batch : 1\n",
      "D Loss : -0.20603403449058533, G Loss : -0.13383939862251282\n",
      "Epoch[659/30000] - Batch : 1\n",
      "D Loss : -0.2060754895210266, G Loss : -0.13421818614006042\n",
      "Epoch[660/30000] - Batch : 1\n",
      "D Loss : -0.20596686005592346, G Loss : -0.13472610712051392\n",
      "Epoch[661/30000] - Batch : 1\n",
      "D Loss : -0.2060137391090393, G Loss : -0.13503217697143555\n",
      "Epoch[662/30000] - Batch : 1\n",
      "D Loss : -0.20615455508232117, G Loss : -0.13519549369812012\n",
      "Epoch[663/30000] - Batch : 1\n",
      "D Loss : -0.20593899488449097, G Loss : -0.13559874892234802\n",
      "Epoch[664/30000] - Batch : 1\n",
      "D Loss : -0.20615197718143463, G Loss : -0.13553191721439362\n",
      "Epoch[665/30000] - Batch : 1\n",
      "D Loss : -0.20609554648399353, G Loss : -0.13567689061164856\n",
      "Epoch[666/30000] - Batch : 1\n",
      "D Loss : -0.2062070071697235, G Loss : -0.13559964299201965\n",
      "Epoch[667/30000] - Batch : 1\n",
      "D Loss : -0.20622000098228455, G Loss : -0.1355816125869751\n",
      "Epoch[668/30000] - Batch : 1\n",
      "D Loss : -0.20625337958335876, G Loss : -0.1354978084564209\n",
      "Epoch[669/30000] - Batch : 1\n",
      "D Loss : -0.2058788388967514, G Loss : -0.13577871024608612\n",
      "Epoch[670/30000] - Batch : 1\n",
      "D Loss : -0.206215500831604, G Loss : -0.13532289862632751\n",
      "Epoch[671/30000] - Batch : 1\n",
      "D Loss : -0.20631521940231323, G Loss : -0.13508182764053345\n",
      "Epoch[672/30000] - Batch : 1\n",
      "D Loss : -0.2037988305091858, G Loss : -0.13727477192878723\n",
      "Epoch[673/30000] - Batch : 1\n",
      "D Loss : -0.20633137226104736, G Loss : -0.13444945216178894\n",
      "Epoch[674/30000] - Batch : 1\n",
      "D Loss : -0.2063678503036499, G Loss : -0.1341547667980194\n",
      "Epoch[675/30000] - Batch : 1\n",
      "D Loss : -0.20581699907779694, G Loss : -0.1344941407442093\n",
      "Epoch[676/30000] - Batch : 1\n",
      "D Loss : -0.20637178421020508, G Loss : -0.13378188014030457\n",
      "Epoch[677/30000] - Batch : 1\n",
      "D Loss : -0.20636487007141113, G Loss : -0.13368618488311768\n",
      "Epoch[678/30000] - Batch : 1\n",
      "D Loss : -0.2064427137374878, G Loss : -0.13354545831680298\n",
      "Epoch[679/30000] - Batch : 1\n",
      "D Loss : -0.2063867151737213, G Loss : -0.1335904896259308\n",
      "Epoch[680/30000] - Batch : 1\n",
      "D Loss : -0.20642121136188507, G Loss : -0.13360093533992767\n",
      "Epoch[681/30000] - Batch : 1\n",
      "D Loss : -0.2064722627401352, G Loss : -0.13364194333553314\n",
      "Epoch[682/30000] - Batch : 1\n",
      "D Loss : -0.20650924742221832, G Loss : -0.13373704254627228\n",
      "Epoch[683/30000] - Batch : 1\n",
      "D Loss : -0.20647890865802765, G Loss : -0.13392247259616852\n",
      "Epoch[684/30000] - Batch : 1\n",
      "D Loss : -0.2065604329109192, G Loss : -0.1340184509754181\n",
      "Epoch[685/30000] - Batch : 1\n",
      "D Loss : -0.20646923780441284, G Loss : -0.134182870388031\n",
      "Epoch[686/30000] - Batch : 1\n",
      "D Loss : -0.20644640922546387, G Loss : -0.13429632782936096\n",
      "Epoch[687/30000] - Batch : 1\n",
      "D Loss : -0.20661556720733643, G Loss : -0.13423296809196472\n",
      "Epoch[688/30000] - Batch : 1\n",
      "D Loss : -0.20660637319087982, G Loss : -0.1343528777360916\n",
      "Epoch[689/30000] - Batch : 1\n",
      "D Loss : -0.20667359232902527, G Loss : -0.13439473509788513\n",
      "Epoch[690/30000] - Batch : 1\n",
      "D Loss : -0.20663875341415405, G Loss : -0.13453063368797302\n",
      "Epoch[691/30000] - Batch : 1\n",
      "D Loss : -0.2067248374223709, G Loss : -0.1345384567975998\n",
      "Epoch[692/30000] - Batch : 1\n",
      "D Loss : -0.20670157670974731, G Loss : -0.13463658094406128\n",
      "Epoch[693/30000] - Batch : 1\n",
      "D Loss : -0.2064419388771057, G Loss : -0.13494715094566345\n",
      "Epoch[694/30000] - Batch : 1\n",
      "D Loss : -0.20682132244110107, G Loss : -0.13460436463356018\n",
      "Epoch[695/30000] - Batch : 1\n",
      "D Loss : -0.2068607211112976, G Loss : -0.13459056615829468\n",
      "Epoch[696/30000] - Batch : 1\n",
      "D Loss : -0.20689328014850616, G Loss : -0.13457272946834564\n",
      "Epoch[697/30000] - Batch : 1\n",
      "D Loss : -0.20680610835552216, G Loss : -0.13461272418498993\n",
      "Epoch[698/30000] - Batch : 1\n",
      "D Loss : -0.20247864723205566, G Loss : -0.13890129327774048\n",
      "Epoch[699/30000] - Batch : 1\n",
      "D Loss : -0.20698261260986328, G Loss : -0.1343599557876587\n",
      "-0.99889934\n",
      "0.9999964\n",
      "0\n",
      "254\n",
      "D Real : [[0.6708172 ]\n",
      " [0.01165701]], D Fake : [[0.12024041]\n",
      " [0.14955664]]\n",
      "Epoch[700/30000] - Batch : 1\n",
      "D Loss : -0.206338569521904, G Loss : -0.13489852845668793\n",
      "Epoch[701/30000] - Batch : 1\n",
      "D Loss : -0.2070431113243103, G Loss : -0.13410955667495728\n",
      "Epoch[702/30000] - Batch : 1\n",
      "D Loss : -0.20690050721168518, G Loss : -0.1341875195503235\n",
      "Epoch[703/30000] - Batch : 1\n",
      "D Loss : -0.20659653842449188, G Loss : -0.13445056974887848\n",
      "Epoch[704/30000] - Batch : 1\n",
      "D Loss : -0.20708255469799042, G Loss : -0.1337629109621048\n",
      "Epoch[705/30000] - Batch : 1\n",
      "D Loss : -0.20706027746200562, G Loss : -0.1336396038532257\n",
      "Epoch[706/30000] - Batch : 1\n",
      "D Loss : -0.2070360779762268, G Loss : -0.1334664225578308\n",
      "Epoch[707/30000] - Batch : 1\n",
      "D Loss : -0.20427504181861877, G Loss : -0.13365668058395386\n",
      "Epoch[708/30000] - Batch : 1\n",
      "D Loss : -0.20448768138885498, G Loss : -0.1336243748664856\n",
      "Epoch[709/30000] - Batch : 1\n",
      "D Loss : -0.20468661189079285, G Loss : -0.13385236263275146\n",
      "Epoch[710/30000] - Batch : 1\n",
      "D Loss : -0.20494693517684937, G Loss : -0.13389360904693604\n",
      "Epoch[711/30000] - Batch : 1\n",
      "D Loss : -0.20498117804527283, G Loss : -0.13408803939819336\n",
      "Epoch[712/30000] - Batch : 1\n",
      "D Loss : -0.20494623482227325, G Loss : -0.13437287509441376\n",
      "Epoch[713/30000] - Batch : 1\n",
      "D Loss : -0.20464394986629486, G Loss : -0.13503079116344452\n",
      "Epoch[714/30000] - Batch : 1\n",
      "D Loss : -0.2050604224205017, G Loss : -0.13497555255889893\n",
      "Epoch[715/30000] - Batch : 1\n",
      "D Loss : -0.2049921154975891, G Loss : -0.13535228371620178\n",
      "Epoch[716/30000] - Batch : 1\n",
      "D Loss : -0.20512428879737854, G Loss : -0.1354936957359314\n",
      "Epoch[717/30000] - Batch : 1\n",
      "D Loss : -0.20517462491989136, G Loss : -0.13566553592681885\n",
      "Epoch[718/30000] - Batch : 1\n",
      "D Loss : -0.20506629347801208, G Loss : -0.13594001531600952\n",
      "Epoch[719/30000] - Batch : 1\n",
      "D Loss : -0.20522788166999817, G Loss : -0.13588660955429077\n",
      "Epoch[720/30000] - Batch : 1\n",
      "D Loss : -0.2051594853401184, G Loss : -0.13602012395858765\n",
      "Epoch[721/30000] - Batch : 1\n",
      "D Loss : -0.20517978072166443, G Loss : -0.13600951433181763\n",
      "Epoch[722/30000] - Batch : 1\n",
      "D Loss : -0.20523545145988464, G Loss : -0.13588812947273254\n",
      "Epoch[723/30000] - Batch : 1\n",
      "D Loss : -0.20535573363304138, G Loss : -0.1356751024723053\n",
      "Epoch[724/30000] - Batch : 1\n",
      "D Loss : -0.20541220903396606, G Loss : -0.13551363348960876\n",
      "Epoch[725/30000] - Batch : 1\n",
      "D Loss : -0.2057981789112091, G Loss : -0.13542112708091736\n",
      "Epoch[726/30000] - Batch : 1\n",
      "D Loss : -0.20652346312999725, G Loss : -0.13590534031391144\n",
      "Epoch[727/30000] - Batch : 1\n",
      "D Loss : -0.20711562037467957, G Loss : -0.1350896656513214\n",
      "Epoch[728/30000] - Batch : 1\n",
      "D Loss : -0.2069854885339737, G Loss : -0.13498561084270477\n",
      "Epoch[729/30000] - Batch : 1\n",
      "D Loss : -0.2072777897119522, G Loss : -0.13445745408535004\n",
      "Epoch[730/30000] - Batch : 1\n",
      "D Loss : -0.20731031894683838, G Loss : -0.13419786095619202\n",
      "Epoch[731/30000] - Batch : 1\n",
      "D Loss : -0.20687629282474518, G Loss : -0.1344272941350937\n",
      "Epoch[732/30000] - Batch : 1\n",
      "D Loss : -0.20736324787139893, G Loss : -0.1337687373161316\n",
      "Epoch[733/30000] - Batch : 1\n",
      "D Loss : -0.20738956332206726, G Loss : -0.1336134672164917\n",
      "Epoch[734/30000] - Batch : 1\n",
      "D Loss : -0.20740988850593567, G Loss : -0.13350334763526917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[735/30000] - Batch : 1\n",
      "D Loss : -0.20741154253482819, G Loss : -0.13346002995967865\n",
      "Epoch[736/30000] - Batch : 1\n",
      "D Loss : -0.20745187997817993, G Loss : -0.13341647386550903\n",
      "Epoch[737/30000] - Batch : 1\n",
      "D Loss : -0.20750005543231964, G Loss : -0.13340891897678375\n",
      "Epoch[738/30000] - Batch : 1\n",
      "D Loss : -0.207536518573761, G Loss : -0.13344892859458923\n",
      "Epoch[739/30000] - Batch : 1\n",
      "D Loss : -0.20757043361663818, G Loss : -0.13348719477653503\n",
      "Epoch[740/30000] - Batch : 1\n",
      "D Loss : -0.20760329067707062, G Loss : -0.13355685770511627\n",
      "Epoch[741/30000] - Batch : 1\n",
      "D Loss : -0.20494863390922546, G Loss : -0.1361379623413086\n",
      "Epoch[742/30000] - Batch : 1\n",
      "D Loss : -0.20764727890491486, G Loss : -0.1334267109632492\n",
      "Epoch[743/30000] - Batch : 1\n",
      "D Loss : -0.20699536800384521, G Loss : -0.1340745985507965\n",
      "Epoch[744/30000] - Batch : 1\n",
      "D Loss : -0.2075732946395874, G Loss : -0.13352680206298828\n",
      "Epoch[745/30000] - Batch : 1\n",
      "D Loss : -0.20770050585269928, G Loss : -0.1334652453660965\n",
      "Epoch[746/30000] - Batch : 1\n",
      "D Loss : -0.20783093571662903, G Loss : -0.13343173265457153\n",
      "Epoch[747/30000] - Batch : 1\n",
      "D Loss : -0.20787253975868225, G Loss : -0.13350293040275574\n",
      "Epoch[748/30000] - Batch : 1\n",
      "D Loss : -0.2069227397441864, G Loss : -0.1345403492450714\n",
      "Epoch[749/30000] - Batch : 1\n",
      "D Loss : -0.20782999694347382, G Loss : -0.13373024761676788\n",
      "Epoch[750/30000] - Batch : 1\n",
      "D Loss : -0.20789414644241333, G Loss : -0.1337718665599823\n",
      "Epoch[751/30000] - Batch : 1\n",
      "D Loss : -0.2079675793647766, G Loss : -0.13382750749588013\n",
      "Epoch[752/30000] - Batch : 1\n",
      "D Loss : -0.20796838402748108, G Loss : -0.1339174211025238\n",
      "Epoch[753/30000] - Batch : 1\n",
      "D Loss : -0.2078639566898346, G Loss : -0.13409993052482605\n",
      "Epoch[754/30000] - Batch : 1\n",
      "D Loss : -0.20593209564685822, G Loss : -0.13602794706821442\n",
      "Epoch[755/30000] - Batch : 1\n",
      "D Loss : -0.20802414417266846, G Loss : -0.13393434882164001\n",
      "Epoch[756/30000] - Batch : 1\n",
      "D Loss : -0.2070537656545639, G Loss : -0.13485141098499298\n",
      "Epoch[757/30000] - Batch : 1\n",
      "D Loss : -0.2080288529396057, G Loss : -0.13382896780967712\n",
      "Epoch[758/30000] - Batch : 1\n",
      "D Loss : -0.20803888142108917, G Loss : -0.13378097116947174\n",
      "Epoch[759/30000] - Batch : 1\n",
      "D Loss : -0.20804353058338165, G Loss : -0.13374914228916168\n",
      "Epoch[760/30000] - Batch : 1\n",
      "D Loss : -0.20804721117019653, G Loss : -0.13372936844825745\n",
      "Epoch[761/30000] - Batch : 1\n",
      "D Loss : -0.20804211497306824, G Loss : -0.13372966647148132\n",
      "Epoch[762/30000] - Batch : 1\n",
      "D Loss : -0.2080356329679489, G Loss : -0.133741095662117\n",
      "Epoch[763/30000] - Batch : 1\n",
      "D Loss : -0.20802754163742065, G Loss : -0.13376227021217346\n",
      "Epoch[764/30000] - Batch : 1\n",
      "D Loss : -0.20805953443050385, G Loss : -0.13374905288219452\n",
      "Epoch[765/30000] - Batch : 1\n",
      "D Loss : -0.20799463987350464, G Loss : -0.13384389877319336\n",
      "Epoch[766/30000] - Batch : 1\n",
      "D Loss : -0.2080691158771515, G Loss : -0.13380196690559387\n",
      "Epoch[767/30000] - Batch : 1\n",
      "D Loss : -0.2080722451210022, G Loss : -0.13383010029792786\n",
      "Epoch[768/30000] - Batch : 1\n",
      "D Loss : -0.2080705314874649, G Loss : -0.13386471569538116\n",
      "Epoch[769/30000] - Batch : 1\n",
      "D Loss : -0.2080479860305786, G Loss : -0.13391700387001038\n",
      "Epoch[770/30000] - Batch : 1\n",
      "D Loss : -0.207638680934906, G Loss : -0.1343267560005188\n",
      "Epoch[771/30000] - Batch : 1\n",
      "D Loss : -0.20805872976779938, G Loss : -0.1339101642370224\n",
      "Epoch[772/30000] - Batch : 1\n",
      "D Loss : -0.20805472135543823, G Loss : -0.13381841778755188\n",
      "Epoch[773/30000] - Batch : 1\n",
      "D Loss : -0.20807264745235443, G Loss : -0.1337287873029709\n",
      "Epoch[774/30000] - Batch : 1\n",
      "D Loss : -0.20798711478710175, G Loss : -0.13375736773014069\n",
      "Epoch[775/30000] - Batch : 1\n",
      "D Loss : -0.2080225646495819, G Loss : -0.1336805820465088\n",
      "Epoch[776/30000] - Batch : 1\n",
      "D Loss : -0.2080799639225006, G Loss : -0.1336022913455963\n",
      "Epoch[777/30000] - Batch : 1\n",
      "D Loss : -0.2078080177307129, G Loss : -0.13387200236320496\n",
      "Epoch[778/30000] - Batch : 1\n",
      "D Loss : -0.20786750316619873, G Loss : -0.13382530212402344\n",
      "Epoch[779/30000] - Batch : 1\n",
      "D Loss : -0.20722821354866028, G Loss : -0.13447940349578857\n",
      "Epoch[780/30000] - Batch : 1\n",
      "D Loss : -0.2080279290676117, G Loss : -0.13370409607887268\n",
      "Epoch[781/30000] - Batch : 1\n",
      "D Loss : -0.20804058015346527, G Loss : -0.13372598588466644\n",
      "Epoch[782/30000] - Batch : 1\n",
      "D Loss : -0.20809009671211243, G Loss : -0.13371852040290833\n",
      "Epoch[783/30000] - Batch : 1\n",
      "D Loss : -0.2081245481967926, G Loss : -0.13372579216957092\n",
      "Epoch[784/30000] - Batch : 1\n",
      "D Loss : -0.20814019441604614, G Loss : -0.13375529646873474\n",
      "Epoch[785/30000] - Batch : 1\n",
      "D Loss : -0.20797522366046906, G Loss : -0.13395775854587555\n",
      "Epoch[786/30000] - Batch : 1\n",
      "D Loss : -0.20815080404281616, G Loss : -0.13382047414779663\n",
      "Epoch[787/30000] - Batch : 1\n",
      "D Loss : -0.20815910398960114, G Loss : -0.1338484138250351\n",
      "Epoch[788/30000] - Batch : 1\n",
      "D Loss : -0.20814558863639832, G Loss : -0.13389351963996887\n",
      "Epoch[789/30000] - Batch : 1\n",
      "D Loss : -0.20814183354377747, G Loss : -0.1339256465435028\n",
      "Epoch[790/30000] - Batch : 1\n",
      "D Loss : -0.20805829763412476, G Loss : -0.13402804732322693\n",
      "Epoch[791/30000] - Batch : 1\n",
      "D Loss : -0.20812976360321045, G Loss : -0.1339723765850067\n",
      "Epoch[792/30000] - Batch : 1\n",
      "D Loss : -0.20810791850090027, G Loss : -0.13398826122283936\n",
      "Epoch[793/30000] - Batch : 1\n",
      "D Loss : -0.2081378549337387, G Loss : -0.13394896686077118\n",
      "Epoch[794/30000] - Batch : 1\n",
      "D Loss : -0.20814424753189087, G Loss : -0.13393175601959229\n",
      "Epoch[795/30000] - Batch : 1\n",
      "D Loss : -0.20811378955841064, G Loss : -0.13394874334335327\n",
      "Epoch[796/30000] - Batch : 1\n",
      "D Loss : -0.2080272138118744, G Loss : -0.1340208351612091\n",
      "Epoch[797/30000] - Batch : 1\n",
      "D Loss : -0.20820820331573486, G Loss : -0.1338283121585846\n",
      "Epoch[798/30000] - Batch : 1\n",
      "D Loss : -0.2081717550754547, G Loss : -0.13385355472564697\n",
      "Epoch[799/30000] - Batch : 1\n",
      "D Loss : -0.20821651816368103, G Loss : -0.1338009238243103\n",
      "-1.0000001\n",
      "1.0\n",
      "0\n",
      "255\n",
      "D Real : [[0.6725485 ]\n",
      " [0.01147557]], D Fake : [[0.12960804]\n",
      " [0.13802043]]\n",
      "Epoch[800/30000] - Batch : 1\n",
      "D Loss : -0.20819778740406036, G Loss : -0.1338142305612564\n",
      "Epoch[801/30000] - Batch : 1\n",
      "D Loss : -0.20703107118606567, G Loss : -0.13494619727134705\n",
      "Epoch[802/30000] - Batch : 1\n",
      "D Loss : -0.20813345909118652, G Loss : -0.133823961019516\n",
      "Epoch[803/30000] - Batch : 1\n",
      "D Loss : -0.2082749307155609, G Loss : -0.13367214798927307\n",
      "Epoch[804/30000] - Batch : 1\n",
      "D Loss : -0.20828303694725037, G Loss : -0.13366347551345825\n",
      "Epoch[805/30000] - Batch : 1\n",
      "D Loss : -0.20824921131134033, G Loss : -0.1337054967880249\n",
      "Epoch[806/30000] - Batch : 1\n",
      "D Loss : -0.2082795351743698, G Loss : -0.13368944823741913\n",
      "Epoch[807/30000] - Batch : 1\n",
      "D Loss : -0.20819321274757385, G Loss : -0.13378706574440002\n",
      "Epoch[808/30000] - Batch : 1\n",
      "D Loss : -0.20832501351833344, G Loss : -0.1336764246225357\n",
      "Epoch[809/30000] - Batch : 1\n",
      "D Loss : -0.20831824839115143, G Loss : -0.13370783627033234\n",
      "Epoch[810/30000] - Batch : 1\n",
      "D Loss : -0.20828448235988617, G Loss : -0.13377036154270172\n",
      "Epoch[811/30000] - Batch : 1\n",
      "D Loss : -0.20834243297576904, G Loss : -0.1337367296218872\n",
      "Epoch[812/30000] - Batch : 1\n",
      "D Loss : -0.208321213722229, G Loss : -0.13378465175628662\n",
      "Epoch[813/30000] - Batch : 1\n",
      "D Loss : -0.20838356018066406, G Loss : -0.13374996185302734\n",
      "Epoch[814/30000] - Batch : 1\n",
      "D Loss : -0.20839612185955048, G Loss : -0.1337638944387436\n",
      "Epoch[815/30000] - Batch : 1\n",
      "D Loss : -0.20841072499752045, G Loss : -0.13377250730991364\n",
      "Epoch[816/30000] - Batch : 1\n",
      "D Loss : -0.2082463651895523, G Loss : -0.13394872844219208\n",
      "Epoch[817/30000] - Batch : 1\n",
      "D Loss : -0.2084406614303589, G Loss : -0.13376346230506897\n",
      "Epoch[818/30000] - Batch : 1\n",
      "D Loss : -0.20845209062099457, G Loss : -0.13376058638095856\n",
      "Epoch[819/30000] - Batch : 1\n",
      "D Loss : -0.20845907926559448, G Loss : -0.1337590515613556\n",
      "Epoch[820/30000] - Batch : 1\n",
      "D Loss : -0.2084866613149643, G Loss : -0.13373513519763947\n",
      "Epoch[821/30000] - Batch : 1\n",
      "D Loss : -0.2084430754184723, G Loss : -0.1337781846523285\n",
      "Epoch[822/30000] - Batch : 1\n",
      "D Loss : -0.2085191011428833, G Loss : -0.13370206952095032\n",
      "Epoch[823/30000] - Batch : 1\n",
      "D Loss : -0.20848090946674347, G Loss : -0.13373993337154388\n",
      "Epoch[824/30000] - Batch : 1\n",
      "D Loss : -0.2085520327091217, G Loss : -0.1336703896522522\n",
      "Epoch[825/30000] - Batch : 1\n",
      "D Loss : -0.2085762619972229, G Loss : -0.1336495280265808\n",
      "Epoch[826/30000] - Batch : 1\n",
      "D Loss : -0.20859336853027344, G Loss : -0.13363879919052124\n",
      "Epoch[827/30000] - Batch : 1\n",
      "D Loss : -0.2085474133491516, G Loss : -0.1336815357208252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[828/30000] - Batch : 1\n",
      "D Loss : -0.2086068093776703, G Loss : -0.13362160325050354\n",
      "Epoch[829/30000] - Batch : 1\n",
      "D Loss : -0.20842769742012024, G Loss : -0.1338065266609192\n",
      "Epoch[830/30000] - Batch : 1\n",
      "D Loss : -0.20785149931907654, G Loss : -0.13436654210090637\n",
      "Epoch[831/30000] - Batch : 1\n",
      "D Loss : -0.20869727432727814, G Loss : -0.1335131973028183\n",
      "Epoch[832/30000] - Batch : 1\n",
      "D Loss : -0.20872029662132263, G Loss : -0.13347232341766357\n",
      "Epoch[833/30000] - Batch : 1\n",
      "D Loss : -0.20874974131584167, G Loss : -0.1334362030029297\n",
      "Epoch[834/30000] - Batch : 1\n",
      "D Loss : -0.2087315320968628, G Loss : -0.13345947861671448\n",
      "Epoch[835/30000] - Batch : 1\n",
      "D Loss : -0.20843248069286346, G Loss : -0.1337626427412033\n",
      "Epoch[836/30000] - Batch : 1\n",
      "D Loss : -0.2086774706840515, G Loss : -0.13352391123771667\n",
      "Epoch[837/30000] - Batch : 1\n",
      "D Loss : -0.20866745710372925, G Loss : -0.13354429602622986\n",
      "Epoch[838/30000] - Batch : 1\n",
      "D Loss : -0.20877644419670105, G Loss : -0.13345912098884583\n",
      "Epoch[839/30000] - Batch : 1\n",
      "D Loss : -0.20889094471931458, G Loss : -0.1334041953086853\n",
      "Epoch[840/30000] - Batch : 1\n",
      "D Loss : -0.20892052352428436, G Loss : -0.13342921435832977\n",
      "Epoch[841/30000] - Batch : 1\n",
      "D Loss : -0.20889678597450256, G Loss : -0.13350525498390198\n",
      "Epoch[842/30000] - Batch : 1\n",
      "D Loss : -0.2087026834487915, G Loss : -0.1337408721446991\n",
      "Epoch[843/30000] - Batch : 1\n",
      "D Loss : -0.20889432728290558, G Loss : -0.13359422981739044\n",
      "Epoch[844/30000] - Batch : 1\n",
      "D Loss : -0.20889630913734436, G Loss : -0.13363522291183472\n",
      "Epoch[845/30000] - Batch : 1\n",
      "D Loss : -0.20903000235557556, G Loss : -0.1335432529449463\n",
      "Epoch[846/30000] - Batch : 1\n",
      "D Loss : -0.2090933918952942, G Loss : -0.13351869583129883\n",
      "Epoch[847/30000] - Batch : 1\n",
      "D Loss : -0.2091258466243744, G Loss : -0.13351553678512573\n",
      "Epoch[848/30000] - Batch : 1\n",
      "D Loss : -0.20883888006210327, G Loss : -0.13382333517074585\n",
      "Epoch[849/30000] - Batch : 1\n",
      "D Loss : -0.20921513438224792, G Loss : -0.1334679126739502\n",
      "Epoch[850/30000] - Batch : 1\n",
      "D Loss : -0.2092285007238388, G Loss : -0.13347302377223969\n",
      "Epoch[851/30000] - Batch : 1\n",
      "D Loss : -0.20929571986198425, G Loss : -0.133424311876297\n",
      "Epoch[852/30000] - Batch : 1\n",
      "D Loss : -0.20932549238204956, G Loss : -0.13341131806373596\n",
      "Epoch[853/30000] - Batch : 1\n",
      "D Loss : -0.20936281979084015, G Loss : -0.13338793814182281\n",
      "Epoch[854/30000] - Batch : 1\n",
      "D Loss : -0.20941761136054993, G Loss : -0.13334858417510986\n",
      "Epoch[855/30000] - Batch : 1\n",
      "D Loss : -0.20939785242080688, G Loss : -0.1333792805671692\n",
      "Epoch[856/30000] - Batch : 1\n",
      "D Loss : -0.20951002836227417, G Loss : -0.13328182697296143\n",
      "Epoch[857/30000] - Batch : 1\n",
      "D Loss : -0.20955267548561096, G Loss : -0.13324064016342163\n",
      "Epoch[858/30000] - Batch : 1\n",
      "D Loss : -0.20958398282527924, G Loss : -0.13320110738277435\n",
      "Epoch[859/30000] - Batch : 1\n",
      "D Loss : -0.2096385955810547, G Loss : -0.13315054774284363\n",
      "Epoch[860/30000] - Batch : 1\n",
      "D Loss : -0.20952892303466797, G Loss : -0.13326045870780945\n",
      "Epoch[861/30000] - Batch : 1\n",
      "D Loss : -0.20965003967285156, G Loss : -0.13314658403396606\n",
      "Epoch[862/30000] - Batch : 1\n",
      "D Loss : -0.20968295633792877, G Loss : -0.13312934339046478\n",
      "Epoch[863/30000] - Batch : 1\n",
      "D Loss : -0.2097456008195877, G Loss : -0.13308967649936676\n",
      "Epoch[864/30000] - Batch : 1\n",
      "D Loss : -0.20974773168563843, G Loss : -0.1331164836883545\n",
      "Epoch[865/30000] - Batch : 1\n",
      "D Loss : -0.20979002118110657, G Loss : -0.13310900330543518\n",
      "Epoch[866/30000] - Batch : 1\n",
      "D Loss : -0.20981895923614502, G Loss : -0.13311946392059326\n",
      "Epoch[867/30000] - Batch : 1\n",
      "D Loss : -0.20966139435768127, G Loss : -0.1332961916923523\n",
      "Epoch[868/30000] - Batch : 1\n",
      "D Loss : -0.20986515283584595, G Loss : -0.13311436772346497\n",
      "Epoch[869/30000] - Batch : 1\n",
      "D Loss : -0.20974630117416382, G Loss : -0.1332579255104065\n",
      "Epoch[870/30000] - Batch : 1\n",
      "D Loss : -0.20976397395133972, G Loss : -0.13326725363731384\n",
      "Epoch[871/30000] - Batch : 1\n",
      "D Loss : -0.20986859500408173, G Loss : -0.13318343460559845\n",
      "Epoch[872/30000] - Batch : 1\n",
      "D Loss : -0.20997031033039093, G Loss : -0.13310478627681732\n",
      "Epoch[873/30000] - Batch : 1\n",
      "D Loss : -0.20990881323814392, G Loss : -0.13318803906440735\n",
      "Epoch[874/30000] - Batch : 1\n",
      "D Loss : -0.21003805100917816, G Loss : -0.13307996094226837\n",
      "Epoch[875/30000] - Batch : 1\n",
      "D Loss : -0.21006615459918976, G Loss : -0.13307444751262665\n",
      "Epoch[876/30000] - Batch : 1\n",
      "D Loss : -0.2100939154624939, G Loss : -0.13306275010108948\n",
      "Epoch[877/30000] - Batch : 1\n",
      "D Loss : -0.21009531617164612, G Loss : -0.13306763768196106\n",
      "Epoch[878/30000] - Batch : 1\n",
      "D Loss : -0.21010291576385498, G Loss : -0.13306358456611633\n",
      "Epoch[879/30000] - Batch : 1\n",
      "D Loss : -0.21007758378982544, G Loss : -0.13309216499328613\n",
      "Epoch[880/30000] - Batch : 1\n",
      "D Loss : -0.21009834110736847, G Loss : -0.1330750733613968\n",
      "Epoch[881/30000] - Batch : 1\n",
      "D Loss : -0.21011151373386383, G Loss : -0.13306547701358795\n",
      "Epoch[882/30000] - Batch : 1\n",
      "D Loss : -0.21010100841522217, G Loss : -0.13306871056556702\n",
      "Epoch[883/30000] - Batch : 1\n",
      "D Loss : -0.21006035804748535, G Loss : -0.13311120867729187\n",
      "Epoch[884/30000] - Batch : 1\n",
      "D Loss : -0.21010714769363403, G Loss : -0.133066326379776\n",
      "Epoch[885/30000] - Batch : 1\n",
      "D Loss : -0.21010303497314453, G Loss : -0.13307425379753113\n",
      "Epoch[886/30000] - Batch : 1\n",
      "D Loss : -0.21011441946029663, G Loss : -0.13306689262390137\n",
      "Epoch[887/30000] - Batch : 1\n",
      "D Loss : -0.2101174294948578, G Loss : -0.13306772708892822\n",
      "Epoch[888/30000] - Batch : 1\n",
      "D Loss : -0.21012738347053528, G Loss : -0.13306137919425964\n",
      "Epoch[889/30000] - Batch : 1\n",
      "D Loss : -0.21011203527450562, G Loss : -0.13307833671569824\n",
      "Epoch[890/30000] - Batch : 1\n",
      "D Loss : -0.2099287211894989, G Loss : -0.13325181603431702\n",
      "Epoch[891/30000] - Batch : 1\n",
      "D Loss : -0.21013811230659485, G Loss : -0.13303229212760925\n",
      "Epoch[892/30000] - Batch : 1\n",
      "D Loss : -0.21013575792312622, G Loss : -0.13299715518951416\n",
      "Epoch[893/30000] - Batch : 1\n",
      "D Loss : -0.20984363555908203, G Loss : -0.1332431435585022\n",
      "Epoch[894/30000] - Batch : 1\n",
      "D Loss : -0.2100910246372223, G Loss : -0.1329573094844818\n",
      "Epoch[895/30000] - Batch : 1\n",
      "D Loss : -0.21015504002571106, G Loss : -0.13287672400474548\n",
      "Epoch[896/30000] - Batch : 1\n",
      "D Loss : -0.21016192436218262, G Loss : -0.1328614056110382\n",
      "Epoch[897/30000] - Batch : 1\n",
      "D Loss : -0.21017524600028992, G Loss : -0.13285183906555176\n",
      "Epoch[898/30000] - Batch : 1\n",
      "D Loss : -0.21017950773239136, G Loss : -0.1328638792037964\n",
      "Epoch[899/30000] - Batch : 1\n",
      "D Loss : -0.21005676686763763, G Loss : -0.13301070034503937\n",
      "-0.99972135\n",
      "0.9999985\n",
      "0\n",
      "254\n",
      "D Real : [[0.6750136 ]\n",
      " [0.01118395]], D Fake : [[0.12970446]\n",
      " [0.1361661 ]]\n",
      "Epoch[900/30000] - Batch : 1\n",
      "D Loss : -0.21016347408294678, G Loss : -0.1329352855682373\n",
      "Epoch[901/30000] - Batch : 1\n",
      "D Loss : -0.21019116044044495, G Loss : -0.13294437527656555\n",
      "Epoch[902/30000] - Batch : 1\n",
      "D Loss : -0.2101902961730957, G Loss : -0.13298487663269043\n",
      "Epoch[903/30000] - Batch : 1\n",
      "D Loss : -0.21020552515983582, G Loss : -0.1330099105834961\n",
      "Epoch[904/30000] - Batch : 1\n",
      "D Loss : -0.2102169692516327, G Loss : -0.1330374777317047\n",
      "Epoch[905/30000] - Batch : 1\n",
      "D Loss : -0.21022851765155792, G Loss : -0.13306106626987457\n",
      "Epoch[906/30000] - Batch : 1\n",
      "D Loss : -0.2102431356906891, G Loss : -0.13307654857635498\n",
      "Epoch[907/30000] - Batch : 1\n",
      "D Loss : -0.21025176346302032, G Loss : -0.13309158384799957\n",
      "Epoch[908/30000] - Batch : 1\n",
      "D Loss : -0.2102561891078949, G Loss : -0.13310417532920837\n",
      "Epoch[909/30000] - Batch : 1\n",
      "D Loss : -0.2100694477558136, G Loss : -0.1332893967628479\n",
      "Epoch[910/30000] - Batch : 1\n",
      "D Loss : -0.2102764993906021, G Loss : -0.13307596743106842\n",
      "Epoch[911/30000] - Batch : 1\n",
      "D Loss : -0.21028847992420197, G Loss : -0.1330542415380478\n",
      "Epoch[912/30000] - Batch : 1\n",
      "D Loss : -0.21026846766471863, G Loss : -0.13306152820587158\n",
      "Epoch[913/30000] - Batch : 1\n",
      "D Loss : -0.21019893884658813, G Loss : -0.1331055760383606\n",
      "Epoch[914/30000] - Batch : 1\n",
      "D Loss : -0.2101345956325531, G Loss : -0.13315048813819885\n",
      "Epoch[915/30000] - Batch : 1\n",
      "D Loss : -0.2103041261434555, G Loss : -0.1329651027917862\n",
      "Epoch[916/30000] - Batch : 1\n",
      "D Loss : -0.2103346884250641, G Loss : -0.13292363286018372\n",
      "Epoch[917/30000] - Batch : 1\n",
      "D Loss : -0.21018260717391968, G Loss : -0.1330602467060089\n",
      "Epoch[918/30000] - Batch : 1\n",
      "D Loss : -0.21033814549446106, G Loss : -0.13288599252700806\n",
      "Epoch[919/30000] - Batch : 1\n",
      "D Loss : -0.21033209562301636, G Loss : -0.13288921117782593\n",
      "Epoch[920/30000] - Batch : 1\n",
      "D Loss : -0.2102210819721222, G Loss : -0.13300183415412903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[921/30000] - Batch : 1\n",
      "D Loss : -0.21037395298480988, G Loss : -0.13285650312900543\n",
      "Epoch[922/30000] - Batch : 1\n",
      "D Loss : -0.2103600800037384, G Loss : -0.13288262486457825\n",
      "Epoch[923/30000] - Batch : 1\n",
      "D Loss : -0.2102697193622589, G Loss : -0.13299047946929932\n",
      "Epoch[924/30000] - Batch : 1\n",
      "D Loss : -0.2103385031223297, G Loss : -0.13294249773025513\n",
      "Epoch[925/30000] - Batch : 1\n",
      "D Loss : -0.21042275428771973, G Loss : -0.13288196921348572\n",
      "Epoch[926/30000] - Batch : 1\n",
      "D Loss : -0.210443913936615, G Loss : -0.13288918137550354\n",
      "Epoch[927/30000] - Batch : 1\n",
      "D Loss : -0.21026557683944702, G Loss : -0.13309821486473083\n",
      "Epoch[928/30000] - Batch : 1\n",
      "D Loss : -0.21047545969486237, G Loss : -0.132918581366539\n",
      "Epoch[929/30000] - Batch : 1\n",
      "D Loss : -0.21041420102119446, G Loss : -0.13300484418869019\n",
      "Epoch[930/30000] - Batch : 1\n",
      "D Loss : -0.21041671931743622, G Loss : -0.13302300870418549\n",
      "Epoch[931/30000] - Batch : 1\n",
      "D Loss : -0.21043160557746887, G Loss : -0.13301962614059448\n",
      "Epoch[932/30000] - Batch : 1\n",
      "D Loss : -0.21050134301185608, G Loss : -0.1329512596130371\n",
      "Epoch[933/30000] - Batch : 1\n",
      "D Loss : -0.21055972576141357, G Loss : -0.13289490342140198\n",
      "Epoch[934/30000] - Batch : 1\n",
      "D Loss : -0.21058949828147888, G Loss : -0.13286271691322327\n",
      "Epoch[935/30000] - Batch : 1\n",
      "D Loss : -0.21060064435005188, G Loss : -0.13285282254219055\n",
      "Epoch[936/30000] - Batch : 1\n",
      "D Loss : -0.21055540442466736, G Loss : -0.1328997015953064\n",
      "Epoch[937/30000] - Batch : 1\n",
      "D Loss : -0.210651695728302, G Loss : -0.13280776143074036\n",
      "Epoch[938/30000] - Batch : 1\n",
      "D Loss : -0.21066652238368988, G Loss : -0.13280119001865387\n",
      "Epoch[939/30000] - Batch : 1\n",
      "D Loss : -0.21043559908866882, G Loss : -0.13303035497665405\n",
      "Epoch[940/30000] - Batch : 1\n",
      "D Loss : -0.2106502652168274, G Loss : -0.1328025460243225\n",
      "Epoch[941/30000] - Batch : 1\n",
      "D Loss : -0.21072280406951904, G Loss : -0.13272562623023987\n",
      "Epoch[942/30000] - Batch : 1\n",
      "D Loss : -0.21076396107673645, G Loss : -0.13268938660621643\n",
      "Epoch[943/30000] - Batch : 1\n",
      "D Loss : -0.2107720673084259, G Loss : -0.13269487023353577\n",
      "Epoch[944/30000] - Batch : 1\n",
      "D Loss : -0.2107991725206375, G Loss : -0.13268570601940155\n",
      "Epoch[945/30000] - Batch : 1\n",
      "D Loss : -0.21046943962574005, G Loss : -0.13302822411060333\n",
      "Epoch[946/30000] - Batch : 1\n",
      "D Loss : -0.21086443960666656, G Loss : -0.1326545625925064\n",
      "Epoch[947/30000] - Batch : 1\n",
      "D Loss : -0.21088173985481262, G Loss : -0.13266733288764954\n",
      "Epoch[948/30000] - Batch : 1\n",
      "D Loss : -0.21092185378074646, G Loss : -0.132663756608963\n",
      "Epoch[949/30000] - Batch : 1\n",
      "D Loss : -0.21084390580654144, G Loss : -0.13276974856853485\n",
      "Epoch[950/30000] - Batch : 1\n",
      "D Loss : -0.21051394939422607, G Loss : -0.13310366868972778\n",
      "Epoch[951/30000] - Batch : 1\n",
      "D Loss : -0.21094921231269836, G Loss : -0.1326771378517151\n",
      "Epoch[952/30000] - Batch : 1\n",
      "D Loss : -0.21081523597240448, G Loss : -0.1328137367963791\n",
      "Epoch[953/30000] - Batch : 1\n",
      "D Loss : -0.21094435453414917, G Loss : -0.1326914131641388\n",
      "Epoch[954/30000] - Batch : 1\n",
      "D Loss : -0.21093842387199402, G Loss : -0.13269886374473572\n",
      "Epoch[955/30000] - Batch : 1\n",
      "D Loss : -0.2109375298023224, G Loss : -0.13270878791809082\n",
      "Epoch[956/30000] - Batch : 1\n",
      "D Loss : -0.21053382754325867, G Loss : -0.133088618516922\n",
      "Epoch[957/30000] - Batch : 1\n",
      "D Loss : -0.21085411310195923, G Loss : -0.13275063037872314\n",
      "Epoch[958/30000] - Batch : 1\n",
      "D Loss : -0.2107989490032196, G Loss : -0.13279342651367188\n",
      "Epoch[959/30000] - Batch : 1\n",
      "D Loss : -0.21093526482582092, G Loss : -0.13265162706375122\n",
      "Epoch[960/30000] - Batch : 1\n",
      "D Loss : -0.21093422174453735, G Loss : -0.13265079259872437\n",
      "Epoch[961/30000] - Batch : 1\n",
      "D Loss : -0.21086393296718597, G Loss : -0.13272549211978912\n",
      "Epoch[962/30000] - Batch : 1\n",
      "D Loss : -0.21093294024467468, G Loss : -0.1326659917831421\n",
      "Epoch[963/30000] - Batch : 1\n",
      "D Loss : -0.21089470386505127, G Loss : -0.13271433115005493\n",
      "Epoch[964/30000] - Batch : 1\n",
      "D Loss : -0.21082991361618042, G Loss : -0.13279220461845398\n",
      "Epoch[965/30000] - Batch : 1\n",
      "D Loss : -0.21087956428527832, G Loss : -0.1327441930770874\n",
      "Epoch[966/30000] - Batch : 1\n",
      "D Loss : -0.21093401312828064, G Loss : -0.132694274187088\n",
      "Epoch[967/30000] - Batch : 1\n",
      "D Loss : -0.2107982039451599, G Loss : -0.13283655047416687\n",
      "Epoch[968/30000] - Batch : 1\n",
      "D Loss : -0.21093785762786865, G Loss : -0.13269701600074768\n",
      "Epoch[969/30000] - Batch : 1\n",
      "D Loss : -0.21093589067459106, G Loss : -0.13269880414009094\n",
      "Epoch[970/30000] - Batch : 1\n",
      "D Loss : -0.20789484679698944, G Loss : -0.13565205037593842\n",
      "Epoch[971/30000] - Batch : 1\n",
      "D Loss : -0.2107551544904709, G Loss : -0.13272725045681\n",
      "Epoch[972/30000] - Batch : 1\n",
      "D Loss : -0.21081386506557465, G Loss : -0.13261888921260834\n",
      "Epoch[973/30000] - Batch : 1\n",
      "D Loss : -0.2109280824661255, G Loss : -0.13247382640838623\n",
      "Epoch[974/30000] - Batch : 1\n",
      "D Loss : -0.21088969707489014, G Loss : -0.13250011205673218\n",
      "Epoch[975/30000] - Batch : 1\n",
      "D Loss : -0.21092432737350464, G Loss : -0.13247144222259521\n",
      "Epoch[976/30000] - Batch : 1\n",
      "D Loss : -0.2109290361404419, G Loss : -0.13249164819717407\n",
      "Epoch[977/30000] - Batch : 1\n",
      "D Loss : -0.21091890335083008, G Loss : -0.13252776861190796\n",
      "Epoch[978/30000] - Batch : 1\n",
      "D Loss : -0.21080882847309113, G Loss : -0.13266845047473907\n",
      "Epoch[979/30000] - Batch : 1\n",
      "D Loss : -0.21084123849868774, G Loss : -0.13267400860786438\n",
      "Epoch[980/30000] - Batch : 1\n",
      "D Loss : -0.2109021246433258, G Loss : -0.1326580047607422\n",
      "Epoch[981/30000] - Batch : 1\n",
      "D Loss : -0.21092242002487183, G Loss : -0.13268396258354187\n",
      "Epoch[982/30000] - Batch : 1\n",
      "D Loss : -0.21091711521148682, G Loss : -0.13273224234580994\n",
      "Epoch[983/30000] - Batch : 1\n",
      "D Loss : -0.21090590953826904, G Loss : -0.13278266787528992\n",
      "Epoch[984/30000] - Batch : 1\n",
      "D Loss : -0.21070857346057892, G Loss : -0.13300932943820953\n",
      "Epoch[985/30000] - Batch : 1\n",
      "D Loss : -0.21077090501785278, G Loss : -0.1329696774482727\n",
      "Epoch[986/30000] - Batch : 1\n",
      "D Loss : -0.21085891127586365, G Loss : -0.13289523124694824\n",
      "Epoch[987/30000] - Batch : 1\n",
      "D Loss : -0.21090558171272278, G Loss : -0.13285338878631592\n",
      "Epoch[988/30000] - Batch : 1\n",
      "D Loss : -0.21090368926525116, G Loss : -0.132852241396904\n",
      "Epoch[989/30000] - Batch : 1\n",
      "D Loss : -0.2108604460954666, G Loss : -0.13285638391971588\n",
      "Epoch[990/30000] - Batch : 1\n",
      "D Loss : -0.2108277976512909, G Loss : -0.1328490674495697\n",
      "Epoch[991/30000] - Batch : 1\n",
      "D Loss : -0.21090610325336456, G Loss : -0.1327332705259323\n",
      "Epoch[992/30000] - Batch : 1\n",
      "D Loss : -0.21074552834033966, G Loss : -0.13285507261753082\n",
      "Epoch[993/30000] - Batch : 1\n",
      "D Loss : -0.21086961030960083, G Loss : -0.1326909065246582\n",
      "Epoch[994/30000] - Batch : 1\n",
      "D Loss : -0.21086910367012024, G Loss : -0.13266798853874207\n",
      "Epoch[995/30000] - Batch : 1\n",
      "D Loss : -0.21088120341300964, G Loss : -0.13264045119285583\n",
      "Epoch[996/30000] - Batch : 1\n",
      "D Loss : -0.21089822053909302, G Loss : -0.13261589407920837\n",
      "Epoch[997/30000] - Batch : 1\n",
      "D Loss : -0.21090258657932281, G Loss : -0.1326158493757248\n",
      "Epoch[998/30000] - Batch : 1\n",
      "D Loss : -0.21090024709701538, G Loss : -0.13263192772865295\n",
      "Epoch[999/30000] - Batch : 1\n",
      "D Loss : -0.21090912818908691, G Loss : -0.1326439082622528\n",
      "-0.9997194\n",
      "0.9981057\n",
      "0\n",
      "254\n",
      "D Real : [[0.01108578]\n",
      " [0.6760704 ]], D Fake : [[0.13278818]\n",
      " [0.13254458]]\n",
      "Epoch[1000/30000] - Batch : 1\n",
      "D Loss : -0.21091172099113464, G Loss : -0.13266637921333313\n",
      "Epoch[1001/30000] - Batch : 1\n",
      "D Loss : -0.210912823677063, G Loss : -0.13269257545471191\n",
      "Epoch[1002/30000] - Batch : 1\n",
      "D Loss : -0.2108365297317505, G Loss : -0.13279414176940918\n",
      "Epoch[1003/30000] - Batch : 1\n",
      "D Loss : -0.2108203023672104, G Loss : -0.13283012807369232\n",
      "Epoch[1004/30000] - Batch : 1\n",
      "D Loss : -0.21090948581695557, G Loss : -0.13275966048240662\n",
      "Epoch[1005/30000] - Batch : 1\n",
      "D Loss : -0.21084342896938324, G Loss : -0.13283954560756683\n",
      "Epoch[1006/30000] - Batch : 1\n",
      "D Loss : -0.2109118103981018, G Loss : -0.13278239965438843\n",
      "Epoch[1007/30000] - Batch : 1\n",
      "D Loss : -0.2109091579914093, G Loss : -0.1327923834323883\n",
      "Epoch[1008/30000] - Batch : 1\n",
      "D Loss : -0.21090193092823029, G Loss : -0.13280294835567474\n",
      "Epoch[1009/30000] - Batch : 1\n",
      "D Loss : -0.2107677310705185, G Loss : -0.13293124735355377\n",
      "Epoch[1010/30000] - Batch : 1\n",
      "D Loss : -0.21091321110725403, G Loss : -0.13277727365493774\n",
      "Epoch[1011/30000] - Batch : 1\n",
      "D Loss : -0.21091300249099731, G Loss : -0.13276726007461548\n",
      "Epoch[1012/30000] - Batch : 1\n",
      "D Loss : -0.2106180489063263, G Loss : -0.13302898406982422\n",
      "Epoch[1013/30000] - Batch : 1\n",
      "D Loss : -0.21005424857139587, G Loss : -0.13356095552444458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1014/30000] - Batch : 1\n",
      "D Loss : -0.21058489382266998, G Loss : -0.1329837292432785\n",
      "Epoch[1015/30000] - Batch : 1\n",
      "D Loss : -0.21037936210632324, G Loss : -0.13316619396209717\n",
      "Epoch[1016/30000] - Batch : 1\n",
      "D Loss : -0.21089661121368408, G Loss : -0.13263556361198425\n",
      "Epoch[1017/30000] - Batch : 1\n",
      "D Loss : -0.21086221933364868, G Loss : -0.13265365362167358\n",
      "Epoch[1018/30000] - Batch : 1\n",
      "D Loss : -0.20887711644172668, G Loss : -0.13462796807289124\n",
      "Epoch[1019/30000] - Batch : 1\n",
      "D Loss : -0.21085888147354126, G Loss : -0.13264325261116028\n",
      "Epoch[1020/30000] - Batch : 1\n",
      "D Loss : -0.21088290214538574, G Loss : -0.13262662291526794\n",
      "Epoch[1021/30000] - Batch : 1\n",
      "D Loss : -0.21085113286972046, G Loss : -0.13267210125923157\n",
      "Epoch[1022/30000] - Batch : 1\n",
      "D Loss : -0.2108837068080902, G Loss : -0.13266000151634216\n",
      "Epoch[1023/30000] - Batch : 1\n",
      "D Loss : -0.2108193188905716, G Loss : -0.13274766504764557\n",
      "Epoch[1024/30000] - Batch : 1\n",
      "D Loss : -0.21077968180179596, G Loss : -0.1327819973230362\n",
      "Epoch[1025/30000] - Batch : 1\n",
      "D Loss : -0.21083003282546997, G Loss : -0.13273173570632935\n",
      "Epoch[1026/30000] - Batch : 1\n",
      "D Loss : -0.21087777614593506, G Loss : -0.1326904594898224\n",
      "Epoch[1027/30000] - Batch : 1\n",
      "D Loss : -0.21086737513542175, G Loss : -0.13270747661590576\n",
      "Epoch[1028/30000] - Batch : 1\n",
      "D Loss : -0.21086755394935608, G Loss : -0.13271784782409668\n",
      "Epoch[1029/30000] - Batch : 1\n",
      "D Loss : -0.2108801007270813, G Loss : -0.13271790742874146\n",
      "Epoch[1030/30000] - Batch : 1\n",
      "D Loss : -0.2108762264251709, G Loss : -0.13273486495018005\n",
      "Epoch[1031/30000] - Batch : 1\n",
      "D Loss : -0.21081402897834778, G Loss : -0.1327032744884491\n",
      "Epoch[1032/30000] - Batch : 1\n",
      "D Loss : -0.21066585183143616, G Loss : -0.13278427720069885\n",
      "Epoch[1033/30000] - Batch : 1\n",
      "D Loss : -0.210859015583992, G Loss : -0.13254229724407196\n",
      "Epoch[1034/30000] - Batch : 1\n",
      "D Loss : -0.21087266504764557, G Loss : -0.13250024616718292\n",
      "Epoch[1035/30000] - Batch : 1\n",
      "D Loss : -0.2108747363090515, G Loss : -0.13248997926712036\n",
      "Epoch[1036/30000] - Batch : 1\n",
      "D Loss : -0.21076640486717224, G Loss : -0.13260850310325623\n",
      "Epoch[1037/30000] - Batch : 1\n",
      "D Loss : -0.21087205410003662, G Loss : -0.13252994418144226\n",
      "Epoch[1038/30000] - Batch : 1\n",
      "D Loss : -0.21087795495986938, G Loss : -0.13256490230560303\n",
      "Epoch[1039/30000] - Batch : 1\n",
      "D Loss : -0.21086359024047852, G Loss : -0.13262948393821716\n",
      "Epoch[1040/30000] - Batch : 1\n",
      "D Loss : -0.21082976460456848, G Loss : -0.132718026638031\n",
      "Epoch[1041/30000] - Batch : 1\n",
      "D Loss : -0.21086052060127258, G Loss : -0.13273966312408447\n",
      "Epoch[1042/30000] - Batch : 1\n",
      "D Loss : -0.21010464429855347, G Loss : -0.13348495960235596\n",
      "Epoch[1043/30000] - Batch : 1\n",
      "D Loss : -0.21076646447181702, G Loss : -0.1328102946281433\n",
      "Epoch[1044/30000] - Batch : 1\n",
      "D Loss : -0.2106821984052658, G Loss : -0.13288240134716034\n",
      "Epoch[1045/30000] - Batch : 1\n",
      "D Loss : -0.2106262743473053, G Loss : -0.13292589783668518\n",
      "Epoch[1046/30000] - Batch : 1\n",
      "D Loss : -0.2108597457408905, G Loss : -0.13268449902534485\n",
      "Epoch[1047/30000] - Batch : 1\n",
      "D Loss : -0.21052314341068268, G Loss : -0.13300864398479462\n",
      "Epoch[1048/30000] - Batch : 1\n",
      "D Loss : -0.2107931524515152, G Loss : -0.13273359835147858\n",
      "Epoch[1049/30000] - Batch : 1\n",
      "D Loss : -0.21085280179977417, G Loss : -0.13266128301620483\n",
      "Epoch[1050/30000] - Batch : 1\n",
      "D Loss : -0.21079090237617493, G Loss : -0.13271990418434143\n",
      "Epoch[1051/30000] - Batch : 1\n",
      "D Loss : -0.21087399125099182, G Loss : -0.13264206051826477\n",
      "Epoch[1052/30000] - Batch : 1\n",
      "D Loss : -0.21085727214813232, G Loss : -0.13266590237617493\n",
      "Epoch[1053/30000] - Batch : 1\n",
      "D Loss : -0.21075093746185303, G Loss : -0.13278359174728394\n",
      "Epoch[1054/30000] - Batch : 1\n",
      "D Loss : -0.21085664629936218, G Loss : -0.13269048929214478\n",
      "Epoch[1055/30000] - Batch : 1\n",
      "D Loss : -0.21083885431289673, G Loss : -0.13270801305770874\n",
      "Epoch[1056/30000] - Batch : 1\n",
      "D Loss : -0.21067190170288086, G Loss : -0.13287240266799927\n",
      "Epoch[1057/30000] - Batch : 1\n",
      "D Loss : -0.21085163950920105, G Loss : -0.13276872038841248\n",
      "Epoch[1058/30000] - Batch : 1\n",
      "D Loss : -0.2108507603406906, G Loss : -0.1327923685312271\n",
      "Epoch[1059/30000] - Batch : 1\n",
      "D Loss : -0.2107224464416504, G Loss : -0.1329202950000763\n",
      "Epoch[1060/30000] - Batch : 1\n",
      "D Loss : -0.21087151765823364, G Loss : -0.1327696442604065\n",
      "Epoch[1061/30000] - Batch : 1\n",
      "D Loss : -0.21063676476478577, G Loss : -0.13299933075904846\n",
      "Epoch[1062/30000] - Batch : 1\n",
      "D Loss : -0.21079424023628235, G Loss : -0.13282284140586853\n",
      "Epoch[1063/30000] - Batch : 1\n",
      "D Loss : -0.21077176928520203, G Loss : -0.13283059000968933\n",
      "Epoch[1064/30000] - Batch : 1\n",
      "D Loss : -0.210536926984787, G Loss : -0.1330331563949585\n",
      "Epoch[1065/30000] - Batch : 1\n",
      "D Loss : -0.2107822597026825, G Loss : -0.13276183605194092\n",
      "Epoch[1066/30000] - Batch : 1\n",
      "D Loss : -0.21085643768310547, G Loss : -0.1326763927936554\n",
      "Epoch[1067/30000] - Batch : 1\n",
      "D Loss : -0.21080327033996582, G Loss : -0.1327039897441864\n",
      "Epoch[1068/30000] - Batch : 1\n",
      "D Loss : -0.21053291857242584, G Loss : -0.13295499980449677\n",
      "Epoch[1069/30000] - Batch : 1\n",
      "D Loss : -0.2108580768108368, G Loss : -0.13261324167251587\n",
      "Epoch[1070/30000] - Batch : 1\n",
      "D Loss : -0.21082164347171783, G Loss : -0.13264630734920502\n",
      "Epoch[1071/30000] - Batch : 1\n",
      "D Loss : -0.21065916121006012, G Loss : -0.13280315697193146\n",
      "Epoch[1072/30000] - Batch : 1\n",
      "D Loss : -0.21086466312408447, G Loss : -0.1326068639755249\n",
      "Epoch[1073/30000] - Batch : 1\n",
      "D Loss : -0.2107486128807068, G Loss : -0.1327323317527771\n",
      "Epoch[1074/30000] - Batch : 1\n",
      "D Loss : -0.21073457598686218, G Loss : -0.132766455411911\n",
      "Epoch[1075/30000] - Batch : 1\n",
      "D Loss : -0.2107754945755005, G Loss : -0.132740318775177\n",
      "Epoch[1076/30000] - Batch : 1\n",
      "D Loss : -0.2108675241470337, G Loss : -0.13267004489898682\n",
      "Epoch[1077/30000] - Batch : 1\n",
      "D Loss : -0.21086645126342773, G Loss : -0.13269737362861633\n",
      "Epoch[1078/30000] - Batch : 1\n",
      "D Loss : -0.21084177494049072, G Loss : -0.13274255394935608\n",
      "Epoch[1079/30000] - Batch : 1\n",
      "D Loss : -0.20998017489910126, G Loss : -0.13360832631587982\n",
      "Epoch[1080/30000] - Batch : 1\n",
      "D Loss : -0.21058419346809387, G Loss : -0.13300296664237976\n",
      "Epoch[1081/30000] - Batch : 1\n",
      "D Loss : -0.2107999324798584, G Loss : -0.13278374075889587\n",
      "Epoch[1082/30000] - Batch : 1\n",
      "D Loss : -0.21086597442626953, G Loss : -0.13271650671958923\n",
      "Epoch[1083/30000] - Batch : 1\n",
      "D Loss : -0.2108490914106369, G Loss : -0.13273276388645172\n",
      "Epoch[1084/30000] - Batch : 1\n",
      "D Loss : -0.21061542630195618, G Loss : -0.13296842575073242\n",
      "Epoch[1085/30000] - Batch : 1\n",
      "D Loss : -0.21086230874061584, G Loss : -0.1327248513698578\n",
      "Epoch[1086/30000] - Batch : 1\n",
      "D Loss : -0.21083877980709076, G Loss : -0.13275550305843353\n",
      "Epoch[1087/30000] - Batch : 1\n",
      "D Loss : -0.21086496114730835, G Loss : -0.13273003697395325\n",
      "Epoch[1088/30000] - Batch : 1\n",
      "D Loss : -0.21083775162696838, G Loss : -0.1327449083328247\n",
      "Epoch[1089/30000] - Batch : 1\n",
      "D Loss : -0.21086576581001282, G Loss : -0.1327110230922699\n",
      "Epoch[1090/30000] - Batch : 1\n",
      "D Loss : -0.2108672559261322, G Loss : -0.132703959941864\n",
      "Epoch[1091/30000] - Batch : 1\n",
      "D Loss : -0.21074476838111877, G Loss : -0.1328216791152954\n",
      "Epoch[1092/30000] - Batch : 1\n",
      "D Loss : -0.21079713106155396, G Loss : -0.13276517391204834\n",
      "Epoch[1093/30000] - Batch : 1\n",
      "D Loss : -0.2108684629201889, G Loss : -0.1326911300420761\n",
      "Epoch[1094/30000] - Batch : 1\n",
      "D Loss : -0.21078960597515106, G Loss : -0.13277174532413483\n",
      "Epoch[1095/30000] - Batch : 1\n",
      "D Loss : -0.21086925268173218, G Loss : -0.1326938271522522\n",
      "Epoch[1096/30000] - Batch : 1\n",
      "D Loss : -0.21067669987678528, G Loss : -0.1328744888305664\n",
      "Epoch[1097/30000] - Batch : 1\n",
      "D Loss : -0.2108657956123352, G Loss : -0.13268056511878967\n",
      "Epoch[1098/30000] - Batch : 1\n",
      "D Loss : -0.21085813641548157, G Loss : -0.13268965482711792\n",
      "Epoch[1099/30000] - Batch : 1\n",
      "D Loss : -0.21083447337150574, G Loss : -0.13271364569664001\n",
      "-1.0\n",
      "1.0\n",
      "0\n",
      "255\n",
      "D Real : [[0.67600024]\n",
      " [0.01109287]], D Fake : [[0.13663724]\n",
      " [0.12883022]]\n",
      "Epoch[1100/30000] - Batch : 1\n",
      "D Loss : -0.21081283688545227, G Loss : -0.13273373246192932\n",
      "Epoch[1101/30000] - Batch : 1\n",
      "D Loss : -0.21085554361343384, G Loss : -0.13267922401428223\n",
      "Epoch[1102/30000] - Batch : 1\n",
      "D Loss : -0.21080605685710907, G Loss : -0.13272620737552643\n",
      "Epoch[1103/30000] - Batch : 1\n",
      "D Loss : -0.21085716784000397, G Loss : -0.13267643749713898\n",
      "Epoch[1104/30000] - Batch : 1\n",
      "D Loss : -0.21085581183433533, G Loss : -0.13268563151359558\n",
      "Epoch[1105/30000] - Batch : 1\n",
      "D Loss : -0.21067163348197937, G Loss : -0.1328800618648529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1106/30000] - Batch : 1\n",
      "D Loss : -0.21084585785865784, G Loss : -0.13270312547683716\n",
      "Epoch[1107/30000] - Batch : 1\n",
      "D Loss : -0.2108326256275177, G Loss : -0.1326993703842163\n",
      "Epoch[1108/30000] - Batch : 1\n",
      "D Loss : -0.21082031726837158, G Loss : -0.13273215293884277\n",
      "Epoch[1109/30000] - Batch : 1\n",
      "D Loss : -0.21084997057914734, G Loss : -0.13271057605743408\n",
      "Epoch[1110/30000] - Batch : 1\n",
      "D Loss : -0.21085502207279205, G Loss : -0.13271479308605194\n",
      "Epoch[1111/30000] - Batch : 1\n",
      "D Loss : -0.21085192263126373, G Loss : -0.13272221386432648\n",
      "Epoch[1112/30000] - Batch : 1\n",
      "D Loss : -0.21083961427211761, G Loss : -0.13274739682674408\n",
      "Epoch[1113/30000] - Batch : 1\n",
      "D Loss : -0.21075350046157837, G Loss : -0.13283994793891907\n",
      "Epoch[1114/30000] - Batch : 1\n",
      "D Loss : -0.21085509657859802, G Loss : -0.1327463686466217\n",
      "Epoch[1115/30000] - Batch : 1\n",
      "D Loss : -0.210820272564888, G Loss : -0.13278917968273163\n",
      "Epoch[1116/30000] - Batch : 1\n",
      "D Loss : -0.2107926309108734, G Loss : -0.13282287120819092\n",
      "Epoch[1117/30000] - Batch : 1\n",
      "D Loss : -0.2108488827943802, G Loss : -0.13277165591716766\n",
      "Epoch[1118/30000] - Batch : 1\n",
      "D Loss : -0.21065446734428406, G Loss : -0.13297107815742493\n",
      "Epoch[1119/30000] - Batch : 1\n",
      "D Loss : -0.21074509620666504, G Loss : -0.13285896182060242\n",
      "Epoch[1120/30000] - Batch : 1\n",
      "D Loss : -0.21084797382354736, G Loss : -0.13273659348487854\n",
      "Epoch[1121/30000] - Batch : 1\n",
      "D Loss : -0.2108515501022339, G Loss : -0.1327187418937683\n",
      "Epoch[1122/30000] - Batch : 1\n",
      "D Loss : -0.21053427457809448, G Loss : -0.1330232322216034\n",
      "Epoch[1123/30000] - Batch : 1\n",
      "D Loss : -0.21079829335212708, G Loss : -0.13275164365768433\n",
      "Epoch[1124/30000] - Batch : 1\n",
      "D Loss : -0.21073435246944427, G Loss : -0.13281212747097015\n",
      "Epoch[1125/30000] - Batch : 1\n",
      "D Loss : -0.21085447072982788, G Loss : -0.13269418478012085\n",
      "Epoch[1126/30000] - Batch : 1\n",
      "D Loss : -0.21085453033447266, G Loss : -0.13270047307014465\n",
      "Epoch[1127/30000] - Batch : 1\n",
      "D Loss : -0.21085482835769653, G Loss : -0.13270819187164307\n",
      "Epoch[1128/30000] - Batch : 1\n",
      "D Loss : -0.21085506677627563, G Loss : -0.13271838426589966\n",
      "Epoch[1129/30000] - Batch : 1\n",
      "D Loss : -0.21085193753242493, G Loss : -0.1327306032180786\n",
      "Epoch[1130/30000] - Batch : 1\n",
      "D Loss : -0.2108338475227356, G Loss : -0.1327584981918335\n",
      "Epoch[1131/30000] - Batch : 1\n",
      "D Loss : -0.21083149313926697, G Loss : -0.1327606737613678\n",
      "Epoch[1132/30000] - Batch : 1\n",
      "D Loss : -0.2108534276485443, G Loss : -0.13274121284484863\n",
      "Epoch[1133/30000] - Batch : 1\n",
      "D Loss : -0.2108536660671234, G Loss : -0.13274535536766052\n",
      "Epoch[1134/30000] - Batch : 1\n",
      "D Loss : -0.21083387732505798, G Loss : -0.13272163271903992\n",
      "Epoch[1135/30000] - Batch : 1\n",
      "D Loss : -0.2108391970396042, G Loss : -0.13268150389194489\n",
      "Epoch[1136/30000] - Batch : 1\n",
      "D Loss : -0.2108241617679596, G Loss : -0.1326691210269928\n",
      "Epoch[1137/30000] - Batch : 1\n",
      "D Loss : -0.2108202576637268, G Loss : -0.13265639543533325\n",
      "Epoch[1138/30000] - Batch : 1\n",
      "D Loss : -0.21083161234855652, G Loss : -0.13263842463493347\n",
      "Epoch[1139/30000] - Batch : 1\n",
      "D Loss : -0.2107999175786972, G Loss : -0.13267342746257782\n",
      "Epoch[1140/30000] - Batch : 1\n",
      "D Loss : -0.21080799400806427, G Loss : -0.13267754018306732\n",
      "Epoch[1141/30000] - Batch : 1\n",
      "D Loss : -0.21082672476768494, G Loss : -0.1326766014099121\n",
      "Epoch[1142/30000] - Batch : 1\n",
      "D Loss : -0.2107858806848526, G Loss : -0.13273634016513824\n",
      "Epoch[1143/30000] - Batch : 1\n",
      "D Loss : -0.21082019805908203, G Loss : -0.13273340463638306\n",
      "Epoch[1144/30000] - Batch : 1\n",
      "D Loss : -0.21032901108264923, G Loss : -0.13320903480052948\n",
      "Epoch[1145/30000] - Batch : 1\n",
      "D Loss : -0.20974747836589813, G Loss : -0.1337336152791977\n",
      "Epoch[1146/30000] - Batch : 1\n",
      "D Loss : -0.21071311831474304, G Loss : -0.13271218538284302\n",
      "Epoch[1147/30000] - Batch : 1\n",
      "D Loss : -0.21077024936676025, G Loss : -0.13262063264846802\n",
      "Epoch[1148/30000] - Batch : 1\n",
      "D Loss : -0.21067799627780914, G Loss : -0.1326955407857895\n",
      "Epoch[1149/30000] - Batch : 1\n",
      "D Loss : -0.21077221632003784, G Loss : -0.1325889229774475\n",
      "Epoch[1150/30000] - Batch : 1\n",
      "D Loss : -0.21080434322357178, G Loss : -0.13256517052650452\n",
      "Epoch[1151/30000] - Batch : 1\n",
      "D Loss : -0.2105194628238678, G Loss : -0.13286492228507996\n",
      "Epoch[1152/30000] - Batch : 1\n",
      "D Loss : -0.21072189509868622, G Loss : -0.13268978893756866\n",
      "Epoch[1153/30000] - Batch : 1\n",
      "D Loss : -0.21080587804317474, G Loss : -0.13264437019824982\n",
      "Epoch[1154/30000] - Batch : 1\n",
      "D Loss : -0.2107696533203125, G Loss : -0.132724791765213\n",
      "Epoch[1155/30000] - Batch : 1\n",
      "D Loss : -0.2107561081647873, G Loss : -0.13278482854366302\n",
      "Epoch[1156/30000] - Batch : 1\n",
      "D Loss : -0.21043112874031067, G Loss : -0.13315072655677795\n",
      "Epoch[1157/30000] - Batch : 1\n",
      "D Loss : -0.21075758337974548, G Loss : -0.13286033272743225\n",
      "Epoch[1158/30000] - Batch : 1\n",
      "D Loss : -0.21081000566482544, G Loss : -0.1328384280204773\n",
      "Epoch[1159/30000] - Batch : 1\n",
      "D Loss : -0.21077758073806763, G Loss : -0.13289278745651245\n",
      "Epoch[1160/30000] - Batch : 1\n",
      "D Loss : -0.21078607439994812, G Loss : -0.13289204239845276\n",
      "Epoch[1161/30000] - Batch : 1\n",
      "D Loss : -0.21075454354286194, G Loss : -0.13292375206947327\n",
      "Epoch[1162/30000] - Batch : 1\n",
      "D Loss : -0.2108246088027954, G Loss : -0.13284814357757568\n",
      "Epoch[1163/30000] - Batch : 1\n",
      "D Loss : -0.2106999009847641, G Loss : -0.13295359909534454\n",
      "Epoch[1164/30000] - Batch : 1\n",
      "D Loss : -0.21077510714530945, G Loss : -0.13285064697265625\n",
      "Epoch[1165/30000] - Batch : 1\n",
      "D Loss : -0.2106907069683075, G Loss : -0.13290399312973022\n",
      "Epoch[1166/30000] - Batch : 1\n",
      "D Loss : -0.21066710352897644, G Loss : -0.13289573788642883\n",
      "Epoch[1167/30000] - Batch : 1\n",
      "D Loss : -0.21082088351249695, G Loss : -0.13271808624267578\n",
      "Epoch[1168/30000] - Batch : 1\n",
      "D Loss : -0.21079161763191223, G Loss : -0.13272160291671753\n",
      "Epoch[1169/30000] - Batch : 1\n",
      "D Loss : -0.2108137309551239, G Loss : -0.13268345594406128\n",
      "Epoch[1170/30000] - Batch : 1\n",
      "D Loss : -0.210810124874115, G Loss : -0.13266396522521973\n",
      "Epoch[1171/30000] - Batch : 1\n",
      "D Loss : -0.2104988843202591, G Loss : -0.13295821845531464\n",
      "Epoch[1172/30000] - Batch : 1\n",
      "D Loss : -0.21082758903503418, G Loss : -0.13262662291526794\n",
      "Epoch[1173/30000] - Batch : 1\n",
      "D Loss : -0.21042516827583313, G Loss : -0.13297566771507263\n",
      "Epoch[1174/30000] - Batch : 1\n",
      "D Loss : -0.21082347631454468, G Loss : -0.13253620266914368\n",
      "Epoch[1175/30000] - Batch : 1\n",
      "D Loss : -0.21075671911239624, G Loss : -0.13257825374603271\n",
      "Epoch[1176/30000] - Batch : 1\n",
      "D Loss : -0.21074694395065308, G Loss : -0.13258516788482666\n",
      "Epoch[1177/30000] - Batch : 1\n",
      "D Loss : -0.2108229696750641, G Loss : -0.13252007961273193\n",
      "Epoch[1178/30000] - Batch : 1\n",
      "D Loss : -0.210823655128479, G Loss : -0.1325494647026062\n",
      "Epoch[1179/30000] - Batch : 1\n",
      "D Loss : -0.20520168542861938, G Loss : -0.13788056373596191\n",
      "Epoch[1180/30000] - Batch : 1\n",
      "D Loss : -0.21072880923748016, G Loss : -0.13213123381137848\n",
      "Epoch[1181/30000] - Batch : 1\n",
      "D Loss : -0.20851965248584747, G Loss : -0.13210444152355194\n",
      "Epoch[1182/30000] - Batch : 1\n",
      "D Loss : -0.20844456553459167, G Loss : -0.13199445605278015\n",
      "Epoch[1183/30000] - Batch : 1\n",
      "D Loss : -0.20856451988220215, G Loss : -0.1321684718132019\n",
      "Epoch[1184/30000] - Batch : 1\n",
      "D Loss : -0.20843623578548431, G Loss : -0.1325884610414505\n",
      "Epoch[1185/30000] - Batch : 1\n",
      "D Loss : -0.2085420936346054, G Loss : -0.13279278576374054\n",
      "Epoch[1186/30000] - Batch : 1\n",
      "D Loss : -0.2082872837781906, G Loss : -0.13338519632816315\n",
      "Epoch[1187/30000] - Batch : 1\n",
      "D Loss : -0.2086143046617508, G Loss : -0.13339324295520782\n",
      "Epoch[1188/30000] - Batch : 1\n",
      "D Loss : -0.20759887993335724, G Loss : -0.1347191482782364\n",
      "Epoch[1189/30000] - Batch : 1\n",
      "D Loss : -0.20817987620830536, G Loss : -0.13443346321582794\n",
      "Epoch[1190/30000] - Batch : 1\n",
      "D Loss : -0.20856645703315735, G Loss : -0.13426503539085388\n",
      "Epoch[1191/30000] - Batch : 1\n",
      "D Loss : -0.20839950442314148, G Loss : -0.13456180691719055\n",
      "Epoch[1192/30000] - Batch : 1\n",
      "D Loss : -0.20867854356765747, G Loss : -0.13437265157699585\n",
      "Epoch[1193/30000] - Batch : 1\n",
      "D Loss : -0.20862144231796265, G Loss : -0.13444602489471436\n",
      "Epoch[1194/30000] - Batch : 1\n",
      "D Loss : -0.20839695632457733, G Loss : -0.13462455570697784\n",
      "Epoch[1195/30000] - Batch : 1\n",
      "D Loss : -0.20842179656028748, G Loss : -0.13449621200561523\n",
      "Epoch[1196/30000] - Batch : 1\n",
      "D Loss : -0.20874127745628357, G Loss : -0.134044349193573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1197/30000] - Batch : 1\n",
      "D Loss : -0.20834240317344666, G Loss : -0.13427647948265076\n",
      "Epoch[1198/30000] - Batch : 1\n",
      "D Loss : -0.20867864787578583, G Loss : -0.13377465307712555\n",
      "Epoch[1199/30000] - Batch : 1\n",
      "D Loss : -0.20879340171813965, G Loss : -0.1335083246231079\n",
      "-0.9998979\n",
      "0.99999917\n",
      "0\n",
      "254\n",
      "D Real : [[0.01137249]\n",
      " [0.67287177]], D Fake : [[0.14974138]\n",
      " [0.11844002]]\n",
      "Epoch[1200/30000] - Batch : 1\n",
      "D Loss : -0.20803144574165344, G Loss : -0.13409069180488586\n",
      "Epoch[1201/30000] - Batch : 1\n",
      "D Loss : -0.20881609618663788, G Loss : -0.13317279517650604\n",
      "Epoch[1202/30000] - Batch : 1\n",
      "D Loss : -0.208769291639328, G Loss : -0.13313087821006775\n",
      "Epoch[1203/30000] - Batch : 1\n",
      "D Loss : -0.2088589370250702, G Loss : -0.13300389051437378\n",
      "Epoch[1204/30000] - Batch : 1\n",
      "D Loss : -0.2088182270526886, G Loss : -0.1330569088459015\n",
      "Epoch[1205/30000] - Batch : 1\n",
      "D Loss : -0.2088945508003235, G Loss : -0.1330384612083435\n",
      "Epoch[1206/30000] - Batch : 1\n",
      "D Loss : -0.20889848470687866, G Loss : -0.1331307291984558\n",
      "Epoch[1207/30000] - Batch : 1\n",
      "D Loss : -0.20891918241977692, G Loss : -0.13323409855365753\n",
      "Epoch[1208/30000] - Batch : 1\n",
      "D Loss : -0.2089131772518158, G Loss : -0.13337820768356323\n",
      "Epoch[1209/30000] - Batch : 1\n",
      "D Loss : -0.20894810557365417, G Loss : -0.1334850788116455\n",
      "Epoch[1210/30000] - Batch : 1\n",
      "D Loss : -0.20898160338401794, G Loss : -0.13358396291732788\n",
      "Epoch[1211/30000] - Batch : 1\n",
      "D Loss : -0.2084239423274994, G Loss : -0.13424590229988098\n",
      "Epoch[1212/30000] - Batch : 1\n",
      "D Loss : -0.20901086926460266, G Loss : -0.13374274969100952\n",
      "Epoch[1213/30000] - Batch : 1\n",
      "D Loss : -0.20899242162704468, G Loss : -0.13369202613830566\n",
      "Epoch[1214/30000] - Batch : 1\n",
      "D Loss : -0.20901313424110413, G Loss : -0.13360819220542908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-52283d0d7e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# tf.trainable_variables()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9ed517095db7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD_optim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dcgan.train()\n",
    "# tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dcgan.d_vars)\n",
    "# print(dcgan.g_vars)\n",
    "# dataset = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataset.get_minibatches(32):\n",
    "#     print(batch.shape)\n",
    "#     print(np.max(batch))\n",
    "#     print(np.min(batch))\n",
    "#     batch = dataset.denormalize(batch)\n",
    "\n",
    "#     batch = batch.astype(np.uint8)\n",
    "#     print(batch.shape)\n",
    "#     print(np.max(batch))\n",
    "#     print(np.min(batch))\n",
    "    \n",
    "#     import matplotlib.pyplot as plt\n",
    "#     plt.figure()\n",
    "#     plt.imshow(batch[5].reshape([28,28]))\n",
    "#     cv2.imwrite('test.png',batch[5])\n",
    "    \n",
    "#     break\n",
    "tf.truncated_normal_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Did not work with mean squared error\n",
    "Use lrelu for G and D - avoids sparse gradients\n",
    "Use average pooling for D - avoids sparse gradients\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
